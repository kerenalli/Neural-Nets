{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210203_Multichannel CNN Model for Text Classification.ipynb",
      "provenance": [],
      "mount_file_id": "175qOvgBwvIZ98dP5k-PSB-xBh_cEAZc9",
      "authorship_tag": "ABX9TyOlXgHT6hD5HFYEwpJzK8S2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerenalli/Neural-Nets/blob/main/20210203_Multichannel_CNN_Model_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePfjAGamuAYD",
        "outputId": "e138eb9e-b8a5-454b-c391-6e11af808334"
      },
      "source": [
        "!python -m nltk.downloader stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfoOLrV9sbWS",
        "outputId": "af51e184-514a-4916-8315-c59371a924c7"
      },
      "source": [
        "from string import punctuation\r\n",
        "from os import listdir\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from pickle import dump\r\n",
        "\r\n",
        "# load doc into memory\r\n",
        "def load_doc(filename):\r\n",
        "\t# open the file as read only\r\n",
        "\tfile = open(filename, 'r')\r\n",
        "\t# read all text\r\n",
        "\ttext = file.read()\r\n",
        "\t# close the file\r\n",
        "\tfile.close()\r\n",
        "\treturn text\r\n",
        "\r\n",
        "# turn a doc into clean tokens\r\n",
        "def clean_doc(doc):\r\n",
        "\t# split into tokens by white space\r\n",
        "\ttokens = doc.split()\r\n",
        "\t# remove punctuation from each token\r\n",
        "\ttable = str.maketrans('', '', punctuation)\r\n",
        "\ttokens = [w.translate(table) for w in tokens]\r\n",
        "\t# remove remaining tokens that are not alphabetic\r\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\r\n",
        "\t# filter out stop words\r\n",
        "\tstop_words = set(stopwords.words('english'))\r\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\r\n",
        "\t# filter out short tokens\r\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\r\n",
        "\ttokens = ' '.join(tokens)\r\n",
        "\treturn tokens\r\n",
        "\r\n",
        "# load all docs in a directory\r\n",
        "def process_docs(directory, is_trian):\r\n",
        "\tdocuments = list()\r\n",
        "\t# walk through all files in the folder\r\n",
        "\tfor filename in listdir(directory):\r\n",
        "\t\t# skip any reviews in the test set\r\n",
        "\t\tif is_trian and filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue\r\n",
        "\t\tif not is_trian and not filename.startswith('cv9'):\r\n",
        "\t\t\tcontinue\r\n",
        "\t\t# create the full path of the file to open\r\n",
        "\t\tpath = directory + '/' + filename\r\n",
        "\t\t# load the doc\r\n",
        "\t\tdoc = load_doc(path)\r\n",
        "\t\t# clean doc\r\n",
        "\t\ttokens = clean_doc(doc)\r\n",
        "\t\t# add to list\r\n",
        "\t\tdocuments.append(tokens)\r\n",
        "\treturn documents\r\n",
        "\r\n",
        "# save a dataset to file\r\n",
        "def save_dataset(dataset, filename):\r\n",
        "\tdump(dataset, open(filename, 'wb'))\r\n",
        "\tprint('Saved: %s' % filename)\r\n",
        "\r\n",
        "# load all training reviews\r\n",
        "negative_docs = process_docs('/content/drive/MyDrive/Data_Set/txt_sentoken/neg', True)\r\n",
        "positive_docs = process_docs('/content/drive/MyDrive/Data_Set/txt_sentoken/pos', True)\r\n",
        "trainX = negative_docs + positive_docs\r\n",
        "trainy = [0 for _ in range(900)] + [1 for _ in range(900)]\r\n",
        "save_dataset([trainX,trainy], 'train.pkl')\r\n",
        "\r\n",
        "# load all test reviews\r\n",
        "negative_docs = process_docs('/content/drive/MyDrive/Data_Set/txt_sentoken/neg', False)\r\n",
        "positive_docs = process_docs('/content/drive/MyDrive/Data_Set/txt_sentoken/pos', False)\r\n",
        "testX = negative_docs + positive_docs\r\n",
        "testY = [0 for _ in range(100)] + [1 for _ in range(100)]\r\n",
        "save_dataset([testX,testY], 'test.pkl')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X_TLVcqu_oh",
        "outputId": "a836702c-e577-4bb1-d7f8-48b73306f7e9"
      },
      "source": [
        "from pickle import load\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Input\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import Embedding\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from keras.layers.merge import concatenate\r\n",
        "\r\n",
        "# load a clean dataset\r\n",
        "def load_dataset(filename):\r\n",
        "\treturn load(open(filename, 'rb'))\r\n",
        "\r\n",
        "# fit a tokenizer\r\n",
        "def create_tokenizer(lines):\r\n",
        "\ttokenizer = Tokenizer()\r\n",
        "\ttokenizer.fit_on_texts(lines)\r\n",
        "\treturn tokenizer\r\n",
        "\r\n",
        "# calculate the maximum document length\r\n",
        "def max_length(lines):\r\n",
        "\treturn max([len(s.split()) for s in lines])\r\n",
        "\r\n",
        "# encode a list of lines\r\n",
        "def encode_text(tokenizer, lines, length):\r\n",
        "\t# integer encode\r\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\r\n",
        "\t# pad encoded sequences\r\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "\treturn padded\r\n",
        "\r\n",
        "# define the model\r\n",
        "def define_model(length, vocab_size):\r\n",
        "\t# channel 1\r\n",
        "\tinputs1 = Input(shape=(length,))\r\n",
        "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\r\n",
        "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\r\n",
        "\tdrop1 = Dropout(0.5)(conv1)\r\n",
        "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\r\n",
        "\tflat1 = Flatten()(pool1)\r\n",
        "\t# channel 2\r\n",
        "\tinputs2 = Input(shape=(length,))\r\n",
        "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\r\n",
        "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\r\n",
        "\tdrop2 = Dropout(0.5)(conv2)\r\n",
        "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\r\n",
        "\tflat2 = Flatten()(pool2)\r\n",
        "\t# channel 3\r\n",
        "\tinputs3 = Input(shape=(length,))\r\n",
        "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\r\n",
        "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\r\n",
        "\tdrop3 = Dropout(0.5)(conv3)\r\n",
        "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\r\n",
        "\tflat3 = Flatten()(pool3)\r\n",
        "\t# merge\r\n",
        "\tmerged = concatenate([flat1, flat2, flat3])\r\n",
        "\t# interpretation\r\n",
        "\tdense1 = Dense(10, activation='relu')(merged)\r\n",
        "\toutputs = Dense(1, activation='sigmoid')(dense1)\r\n",
        "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\r\n",
        "\t# compile\r\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\t# summarize\r\n",
        "\tprint(model.summary())\r\n",
        "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\r\n",
        "\treturn model\r\n",
        "\r\n",
        "# load training dataset\r\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\r\n",
        "# create tokenizer\r\n",
        "tokenizer = create_tokenizer(trainLines)\r\n",
        "# calculate max document length\r\n",
        "length = max_length(trainLines)\r\n",
        "# calculate vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "# encode data\r\n",
        "trainX = encode_text(tokenizer, trainLines, length)\r\n",
        "print(trainX.shape)\r\n",
        "\r\n",
        "# define model\r\n",
        "model = define_model(length, vocab_size)\r\n",
        "# fit model\r\n",
        "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\r\n",
        "# save the model\r\n",
        "model.save('model.h5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 1380\n",
            "Vocabulary size: 44277\n",
            "(1800, 1380)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1380)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1380)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1380)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1380, 100)    4427700     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 1380, 100)    4427700     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1380, 100)    4427700     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 1377, 32)     12832       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 1375, 32)     19232       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 1373, 32)     25632       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1377, 32)     0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1375, 32)     0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 1373, 32)     0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 688, 32)      0           dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 687, 32)      0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 686, 32)      0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 22016)        0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 21984)        0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 21952)        0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 65952)        0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           659530      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 14,000,337\n",
            "Trainable params: 14,000,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "113/113 [==============================] - 23s 130ms/step - loss: 0.6960 - accuracy: 0.5023\n",
            "Epoch 2/10\n",
            "113/113 [==============================] - 14s 128ms/step - loss: 0.5332 - accuracy: 0.7550\n",
            "Epoch 3/10\n",
            "113/113 [==============================] - 15s 128ms/step - loss: 0.0538 - accuracy: 0.9890\n",
            "Epoch 4/10\n",
            "113/113 [==============================] - 14s 128ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "113/113 [==============================] - 15s 129ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "113/113 [==============================] - 15s 129ms/step - loss: 7.1422e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "113/113 [==============================] - 15s 129ms/step - loss: 5.0708e-04 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "113/113 [==============================] - 15s 130ms/step - loss: 2.8073e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "113/113 [==============================] - 15s 131ms/step - loss: 2.2696e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "113/113 [==============================] - 15s 129ms/step - loss: 2.2907e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uteg_6HTvmKT",
        "outputId": "8e548fbc-0da5-46c9-8bca-9af0b76c8e3b"
      },
      "source": [
        "from pickle import load\r\n",
        "from numpy import array\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import load_model\r\n",
        "\r\n",
        "# load a clean dataset\r\n",
        "def load_dataset(filename):\r\n",
        "\treturn load(open(filename, 'rb'))\r\n",
        "\r\n",
        "# fit a tokenizer\r\n",
        "def create_tokenizer(lines):\r\n",
        "\ttokenizer = Tokenizer()\r\n",
        "\ttokenizer.fit_on_texts(lines)\r\n",
        "\treturn tokenizer\r\n",
        "\r\n",
        "# calculate the maximum document length\r\n",
        "def max_length(lines):\r\n",
        "\treturn max([len(s.split()) for s in lines])\r\n",
        "\r\n",
        "# encode a list of lines\r\n",
        "def encode_text(tokenizer, lines, length):\r\n",
        "\t# integer encode\r\n",
        "\tencoded = tokenizer.texts_to_sequences(lines)\r\n",
        "\t# pad encoded sequences\r\n",
        "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\r\n",
        "\treturn padded\r\n",
        "\r\n",
        "# load datasets\r\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\r\n",
        "testLines, testLabels = load_dataset('test.pkl')\r\n",
        "\r\n",
        "# create tokenizer\r\n",
        "tokenizer = create_tokenizer(trainLines)\r\n",
        "# calculate max document length\r\n",
        "length = max_length(trainLines)\r\n",
        "# calculate vocabulary size\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Max document length: %d' % length)\r\n",
        "print('Vocabulary size: %d' % vocab_size)\r\n",
        "# encode data\r\n",
        "trainX = encode_text(tokenizer, trainLines, length)\r\n",
        "testX = encode_text(tokenizer, testLines, length)\r\n",
        "print(trainX.shape, testX.shape)\r\n",
        "\r\n",
        "# load the model\r\n",
        "model = load_model('model.h5')\r\n",
        "\r\n",
        "# evaluate model on training dataset\r\n",
        "loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\r\n",
        "print('Train Accuracy: %f' % (acc*100))\r\n",
        "\r\n",
        "# evaluate model on test dataset dataset\r\n",
        "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\r\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max document length: 1380\n",
            "Vocabulary size: 44277\n",
            "(1800, 1380) (200, 1380)\n",
            "Train Accuracy: 100.000000\n",
            "Test Accuracy: 85.000002\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}