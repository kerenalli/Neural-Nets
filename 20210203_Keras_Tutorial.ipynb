{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210203_Keras_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11MyEAm_I5TN1-M1cYlcbY3V7Bt6uWE15",
      "authorship_tag": "ABX9TyPA9KPJLWdD4AKu9ZWdArqe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerenalli/Neural-Nets/blob/main/20210203_Keras_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHqGmGP8SY3m",
        "outputId": "ed3cbddc-6d5e-40b1-a4c5-79318512c95e"
      },
      "source": [
        "# first neural network with keras tutorial\r\n",
        "from numpy import loadtxt\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "# load the dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "# split into input (X) and output (y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# define the keras model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# compile the keras model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# fit the keras model on the dataset\r\n",
        "model.fit(X, y, epochs=150, batch_size=10)\r\n",
        "# evaluate the keras model\r\n",
        "_, accuracy = model.evaluate(X, y)\r\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 2s 3ms/step - loss: 26.6851 - accuracy: 0.3641\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 9.3130 - accuracy: 0.5109\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 3.2156 - accuracy: 0.6306\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 1.6301 - accuracy: 0.6427\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 1.1765 - accuracy: 0.6541\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 1.0022 - accuracy: 0.6421\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8658 - accuracy: 0.6554\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.8647 - accuracy: 0.6500\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.7947 - accuracy: 0.6691\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6935 - accuracy: 0.6918\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.6873\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6858 - accuracy: 0.6897\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6307 - accuracy: 0.6892\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.6536 - accuracy: 0.6739\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.6963\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6379 - accuracy: 0.6797\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6348 - accuracy: 0.6789\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6077 - accuracy: 0.7001\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6170 - accuracy: 0.6852\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5814 - accuracy: 0.7165\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5624 - accuracy: 0.7294\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.6978\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5998 - accuracy: 0.6797\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6252 - accuracy: 0.6605\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.6954\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.7188\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.7246\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5940 - accuracy: 0.7087\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.6081 - accuracy: 0.7117\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5718 - accuracy: 0.7134\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5819 - accuracy: 0.7064\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5411 - accuracy: 0.7497\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5600 - accuracy: 0.7267\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5831 - accuracy: 0.7220\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7152\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5710 - accuracy: 0.7006\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5435 - accuracy: 0.7190\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5673 - accuracy: 0.7250\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5521 - accuracy: 0.7279\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5670 - accuracy: 0.7211\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5604 - accuracy: 0.7232\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5884 - accuracy: 0.7030\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5456 - accuracy: 0.7229\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5419 - accuracy: 0.7316\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5398 - accuracy: 0.7519\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7354\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5789 - accuracy: 0.6977\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5591 - accuracy: 0.7202\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5499 - accuracy: 0.7237\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7155\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7203\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5860 - accuracy: 0.7032\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7384\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5365 - accuracy: 0.7110\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5850 - accuracy: 0.6937\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5128 - accuracy: 0.7552\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.6957\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5398 - accuracy: 0.7259\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5408 - accuracy: 0.7507\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7112\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5241 - accuracy: 0.7428\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5252 - accuracy: 0.7573\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5554 - accuracy: 0.7372\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5494 - accuracy: 0.7231\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5225 - accuracy: 0.7613\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7497\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5185 - accuracy: 0.7509\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5296 - accuracy: 0.7511\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5121 - accuracy: 0.7680\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.7186\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5230 - accuracy: 0.7518\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5192 - accuracy: 0.7537\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5567 - accuracy: 0.7097\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7621\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5044 - accuracy: 0.7555\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5469 - accuracy: 0.7341\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5273 - accuracy: 0.7334\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5242 - accuracy: 0.7285\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4959 - accuracy: 0.7788\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.7309\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7194\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7640\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5327 - accuracy: 0.7287\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7582\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5407 - accuracy: 0.7188\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5199 - accuracy: 0.7452\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4891 - accuracy: 0.7650\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7417\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.7221\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4922 - accuracy: 0.7716\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5258 - accuracy: 0.7247\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.7498\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.7944\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5086 - accuracy: 0.7462\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5002 - accuracy: 0.7630\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.7634\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7482\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.7762\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7593\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7713\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4806 - accuracy: 0.7653\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5122 - accuracy: 0.7496\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5069 - accuracy: 0.7321\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5101 - accuracy: 0.7350\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5355 - accuracy: 0.7227\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5031 - accuracy: 0.7331\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7615\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5025 - accuracy: 0.7439\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4970 - accuracy: 0.7674\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5041 - accuracy: 0.7542\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5497 - accuracy: 0.7286\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.7730\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4906 - accuracy: 0.7639\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.7477\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7457\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4944 - accuracy: 0.7644\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5144 - accuracy: 0.7554\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.7983\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4943 - accuracy: 0.7588\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.7703\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5166 - accuracy: 0.7615\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4643 - accuracy: 0.7855\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7501\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7630\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4893 - accuracy: 0.7478\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.4939 - accuracy: 0.7559\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.7855\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4746 - accuracy: 0.7483\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4185 - accuracy: 0.8094\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4636 - accuracy: 0.7701\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5063 - accuracy: 0.7346\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.7862\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7642\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4966 - accuracy: 0.7722\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4865 - accuracy: 0.7519\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.7655\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5200 - accuracy: 0.7398\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7747\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4564 - accuracy: 0.7772\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7801\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4689 - accuracy: 0.7428\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5301 - accuracy: 0.7372\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4743 - accuracy: 0.7886\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.4894 - accuracy: 0.7661\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 4ms/step - loss: 0.5179 - accuracy: 0.7188\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7777\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.7924\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.7676\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.7841\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.7639\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7487\n",
            "Accuracy: 74.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "25IaTsMQNvDG",
        "outputId": "0ef6c875-ac1a-45dd-d258-eeeb18f7be31"
      },
      "source": [
        "# train autoencoder for regression with no compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_regression\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import ReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = ReLU()(e)\r\n",
        "# define bottleneck\r\n",
        "n_bottleneck = n_inputs\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder\r\n",
        "d = Dense(n_inputs*2)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = ReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=400, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "39/39 - 1s - loss: 0.5232 - val_loss: 0.1249\n",
            "Epoch 2/400\n",
            "39/39 - 0s - loss: 0.2653 - val_loss: 0.1109\n",
            "Epoch 3/400\n",
            "39/39 - 0s - loss: 0.1441 - val_loss: 0.0965\n",
            "Epoch 4/400\n",
            "39/39 - 0s - loss: 0.0952 - val_loss: 0.0824\n",
            "Epoch 5/400\n",
            "39/39 - 0s - loss: 0.0663 - val_loss: 0.0695\n",
            "Epoch 6/400\n",
            "39/39 - 0s - loss: 0.0542 - val_loss: 0.0584\n",
            "Epoch 7/400\n",
            "39/39 - 0s - loss: 0.0459 - val_loss: 0.0494\n",
            "Epoch 8/400\n",
            "39/39 - 0s - loss: 0.0389 - val_loss: 0.0404\n",
            "Epoch 9/400\n",
            "39/39 - 0s - loss: 0.0349 - val_loss: 0.0344\n",
            "Epoch 10/400\n",
            "39/39 - 0s - loss: 0.0316 - val_loss: 0.0292\n",
            "Epoch 11/400\n",
            "39/39 - 0s - loss: 0.0298 - val_loss: 0.0252\n",
            "Epoch 12/400\n",
            "39/39 - 0s - loss: 0.0266 - val_loss: 0.0229\n",
            "Epoch 13/400\n",
            "39/39 - 0s - loss: 0.0253 - val_loss: 0.0209\n",
            "Epoch 14/400\n",
            "39/39 - 0s - loss: 0.0244 - val_loss: 0.0201\n",
            "Epoch 15/400\n",
            "39/39 - 0s - loss: 0.0235 - val_loss: 0.0188\n",
            "Epoch 16/400\n",
            "39/39 - 0s - loss: 0.0220 - val_loss: 0.0181\n",
            "Epoch 17/400\n",
            "39/39 - 0s - loss: 0.0211 - val_loss: 0.0173\n",
            "Epoch 18/400\n",
            "39/39 - 0s - loss: 0.0198 - val_loss: 0.0169\n",
            "Epoch 19/400\n",
            "39/39 - 0s - loss: 0.0192 - val_loss: 0.0159\n",
            "Epoch 20/400\n",
            "39/39 - 0s - loss: 0.0176 - val_loss: 0.0156\n",
            "Epoch 21/400\n",
            "39/39 - 0s - loss: 0.0178 - val_loss: 0.0150\n",
            "Epoch 22/400\n",
            "39/39 - 0s - loss: 0.0173 - val_loss: 0.0145\n",
            "Epoch 23/400\n",
            "39/39 - 0s - loss: 0.0165 - val_loss: 0.0142\n",
            "Epoch 24/400\n",
            "39/39 - 0s - loss: 0.0160 - val_loss: 0.0141\n",
            "Epoch 25/400\n",
            "39/39 - 0s - loss: 0.0169 - val_loss: 0.0139\n",
            "Epoch 26/400\n",
            "39/39 - 0s - loss: 0.0165 - val_loss: 0.0135\n",
            "Epoch 27/400\n",
            "39/39 - 0s - loss: 0.0160 - val_loss: 0.0134\n",
            "Epoch 28/400\n",
            "39/39 - 0s - loss: 0.0155 - val_loss: 0.0130\n",
            "Epoch 29/400\n",
            "39/39 - 0s - loss: 0.0152 - val_loss: 0.0127\n",
            "Epoch 30/400\n",
            "39/39 - 0s - loss: 0.0149 - val_loss: 0.0124\n",
            "Epoch 31/400\n",
            "39/39 - 0s - loss: 0.0143 - val_loss: 0.0124\n",
            "Epoch 32/400\n",
            "39/39 - 0s - loss: 0.0140 - val_loss: 0.0119\n",
            "Epoch 33/400\n",
            "39/39 - 0s - loss: 0.0140 - val_loss: 0.0117\n",
            "Epoch 34/400\n",
            "39/39 - 0s - loss: 0.0133 - val_loss: 0.0114\n",
            "Epoch 35/400\n",
            "39/39 - 0s - loss: 0.0131 - val_loss: 0.0111\n",
            "Epoch 36/400\n",
            "39/39 - 0s - loss: 0.0123 - val_loss: 0.0111\n",
            "Epoch 37/400\n",
            "39/39 - 0s - loss: 0.0136 - val_loss: 0.0108\n",
            "Epoch 38/400\n",
            "39/39 - 0s - loss: 0.0126 - val_loss: 0.0108\n",
            "Epoch 39/400\n",
            "39/39 - 0s - loss: 0.0124 - val_loss: 0.0104\n",
            "Epoch 40/400\n",
            "39/39 - 0s - loss: 0.0128 - val_loss: 0.0103\n",
            "Epoch 41/400\n",
            "39/39 - 0s - loss: 0.0122 - val_loss: 0.0103\n",
            "Epoch 42/400\n",
            "39/39 - 0s - loss: 0.0122 - val_loss: 0.0101\n",
            "Epoch 43/400\n",
            "39/39 - 0s - loss: 0.0124 - val_loss: 0.0101\n",
            "Epoch 44/400\n",
            "39/39 - 0s - loss: 0.0116 - val_loss: 0.0100\n",
            "Epoch 45/400\n",
            "39/39 - 0s - loss: 0.0112 - val_loss: 0.0097\n",
            "Epoch 46/400\n",
            "39/39 - 0s - loss: 0.0115 - val_loss: 0.0096\n",
            "Epoch 47/400\n",
            "39/39 - 0s - loss: 0.0110 - val_loss: 0.0097\n",
            "Epoch 48/400\n",
            "39/39 - 0s - loss: 0.0110 - val_loss: 0.0093\n",
            "Epoch 49/400\n",
            "39/39 - 0s - loss: 0.0107 - val_loss: 0.0092\n",
            "Epoch 50/400\n",
            "39/39 - 0s - loss: 0.0105 - val_loss: 0.0092\n",
            "Epoch 51/400\n",
            "39/39 - 0s - loss: 0.0104 - val_loss: 0.0093\n",
            "Epoch 52/400\n",
            "39/39 - 0s - loss: 0.0104 - val_loss: 0.0088\n",
            "Epoch 53/400\n",
            "39/39 - 0s - loss: 0.0100 - val_loss: 0.0087\n",
            "Epoch 54/400\n",
            "39/39 - 0s - loss: 0.0103 - val_loss: 0.0088\n",
            "Epoch 55/400\n",
            "39/39 - 0s - loss: 0.0100 - val_loss: 0.0085\n",
            "Epoch 56/400\n",
            "39/39 - 0s - loss: 0.0095 - val_loss: 0.0083\n",
            "Epoch 57/400\n",
            "39/39 - 0s - loss: 0.0100 - val_loss: 0.0084\n",
            "Epoch 58/400\n",
            "39/39 - 0s - loss: 0.0102 - val_loss: 0.0082\n",
            "Epoch 59/400\n",
            "39/39 - 0s - loss: 0.0102 - val_loss: 0.0084\n",
            "Epoch 60/400\n",
            "39/39 - 0s - loss: 0.0104 - val_loss: 0.0081\n",
            "Epoch 61/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0078\n",
            "Epoch 62/400\n",
            "39/39 - 0s - loss: 0.0090 - val_loss: 0.0077\n",
            "Epoch 63/400\n",
            "39/39 - 0s - loss: 0.0097 - val_loss: 0.0077\n",
            "Epoch 64/400\n",
            "39/39 - 0s - loss: 0.0093 - val_loss: 0.0077\n",
            "Epoch 65/400\n",
            "39/39 - 0s - loss: 0.0094 - val_loss: 0.0075\n",
            "Epoch 66/400\n",
            "39/39 - 0s - loss: 0.0086 - val_loss: 0.0073\n",
            "Epoch 67/400\n",
            "39/39 - 0s - loss: 0.0092 - val_loss: 0.0073\n",
            "Epoch 68/400\n",
            "39/39 - 0s - loss: 0.0089 - val_loss: 0.0072\n",
            "Epoch 69/400\n",
            "39/39 - 0s - loss: 0.0088 - val_loss: 0.0069\n",
            "Epoch 70/400\n",
            "39/39 - 0s - loss: 0.0089 - val_loss: 0.0073\n",
            "Epoch 71/400\n",
            "39/39 - 0s - loss: 0.0093 - val_loss: 0.0068\n",
            "Epoch 72/400\n",
            "39/39 - 0s - loss: 0.0095 - val_loss: 0.0071\n",
            "Epoch 73/400\n",
            "39/39 - 0s - loss: 0.0083 - val_loss: 0.0068\n",
            "Epoch 74/400\n",
            "39/39 - 0s - loss: 0.0086 - val_loss: 0.0066\n",
            "Epoch 75/400\n",
            "39/39 - 0s - loss: 0.0083 - val_loss: 0.0065\n",
            "Epoch 76/400\n",
            "39/39 - 0s - loss: 0.0082 - val_loss: 0.0065\n",
            "Epoch 77/400\n",
            "39/39 - 0s - loss: 0.0085 - val_loss: 0.0064\n",
            "Epoch 78/400\n",
            "39/39 - 0s - loss: 0.0082 - val_loss: 0.0064\n",
            "Epoch 79/400\n",
            "39/39 - 0s - loss: 0.0083 - val_loss: 0.0062\n",
            "Epoch 80/400\n",
            "39/39 - 0s - loss: 0.0085 - val_loss: 0.0062\n",
            "Epoch 81/400\n",
            "39/39 - 0s - loss: 0.0080 - val_loss: 0.0062\n",
            "Epoch 82/400\n",
            "39/39 - 0s - loss: 0.0085 - val_loss: 0.0063\n",
            "Epoch 83/400\n",
            "39/39 - 0s - loss: 0.0080 - val_loss: 0.0060\n",
            "Epoch 84/400\n",
            "39/39 - 0s - loss: 0.0081 - val_loss: 0.0059\n",
            "Epoch 85/400\n",
            "39/39 - 0s - loss: 0.0079 - val_loss: 0.0059\n",
            "Epoch 86/400\n",
            "39/39 - 0s - loss: 0.0081 - val_loss: 0.0059\n",
            "Epoch 87/400\n",
            "39/39 - 0s - loss: 0.0077 - val_loss: 0.0058\n",
            "Epoch 88/400\n",
            "39/39 - 0s - loss: 0.0079 - val_loss: 0.0057\n",
            "Epoch 89/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0056\n",
            "Epoch 90/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0055\n",
            "Epoch 91/400\n",
            "39/39 - 0s - loss: 0.0072 - val_loss: 0.0055\n",
            "Epoch 92/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0058\n",
            "Epoch 93/400\n",
            "39/39 - 0s - loss: 0.0079 - val_loss: 0.0055\n",
            "Epoch 94/400\n",
            "39/39 - 0s - loss: 0.0076 - val_loss: 0.0055\n",
            "Epoch 95/400\n",
            "39/39 - 0s - loss: 0.0077 - val_loss: 0.0055\n",
            "Epoch 96/400\n",
            "39/39 - 0s - loss: 0.0076 - val_loss: 0.0054\n",
            "Epoch 97/400\n",
            "39/39 - 0s - loss: 0.0079 - val_loss: 0.0056\n",
            "Epoch 98/400\n",
            "39/39 - 0s - loss: 0.0073 - val_loss: 0.0052\n",
            "Epoch 99/400\n",
            "39/39 - 0s - loss: 0.0070 - val_loss: 0.0052\n",
            "Epoch 100/400\n",
            "39/39 - 0s - loss: 0.0077 - val_loss: 0.0053\n",
            "Epoch 101/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0053\n",
            "Epoch 102/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0050\n",
            "Epoch 103/400\n",
            "39/39 - 0s - loss: 0.0073 - val_loss: 0.0051\n",
            "Epoch 104/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0052\n",
            "Epoch 105/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0052\n",
            "Epoch 106/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0050\n",
            "Epoch 107/400\n",
            "39/39 - 0s - loss: 0.0071 - val_loss: 0.0051\n",
            "Epoch 108/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0050\n",
            "Epoch 109/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0049\n",
            "Epoch 110/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0050\n",
            "Epoch 111/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0047\n",
            "Epoch 112/400\n",
            "39/39 - 0s - loss: 0.0072 - val_loss: 0.0048\n",
            "Epoch 113/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0048\n",
            "Epoch 114/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0048\n",
            "Epoch 115/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0048\n",
            "Epoch 116/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0046\n",
            "Epoch 117/400\n",
            "39/39 - 0s - loss: 0.0065 - val_loss: 0.0046\n",
            "Epoch 118/400\n",
            "39/39 - 0s - loss: 0.0066 - val_loss: 0.0047\n",
            "Epoch 119/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0046\n",
            "Epoch 120/400\n",
            "39/39 - 0s - loss: 0.0065 - val_loss: 0.0047\n",
            "Epoch 121/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0045\n",
            "Epoch 122/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0044\n",
            "Epoch 123/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0044\n",
            "Epoch 124/400\n",
            "39/39 - 0s - loss: 0.0065 - val_loss: 0.0043\n",
            "Epoch 125/400\n",
            "39/39 - 0s - loss: 0.0066 - val_loss: 0.0043\n",
            "Epoch 126/400\n",
            "39/39 - 0s - loss: 0.0066 - val_loss: 0.0042\n",
            "Epoch 127/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0046\n",
            "Epoch 128/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0042\n",
            "Epoch 129/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0043\n",
            "Epoch 130/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0042\n",
            "Epoch 131/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0044\n",
            "Epoch 132/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0041\n",
            "Epoch 133/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0042\n",
            "Epoch 134/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0042\n",
            "Epoch 135/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0041\n",
            "Epoch 136/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0042\n",
            "Epoch 137/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 138/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0045\n",
            "Epoch 139/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0042\n",
            "Epoch 140/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0040\n",
            "Epoch 141/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0039\n",
            "Epoch 142/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0039\n",
            "Epoch 143/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0037\n",
            "Epoch 144/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0038\n",
            "Epoch 145/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0038\n",
            "Epoch 146/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0036\n",
            "Epoch 147/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0038\n",
            "Epoch 148/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0037\n",
            "Epoch 149/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0037\n",
            "Epoch 150/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0036\n",
            "Epoch 151/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0035\n",
            "Epoch 152/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0035\n",
            "Epoch 153/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0037\n",
            "Epoch 154/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0037\n",
            "Epoch 155/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0034\n",
            "Epoch 156/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0035\n",
            "Epoch 157/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0035\n",
            "Epoch 158/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0035\n",
            "Epoch 159/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0034\n",
            "Epoch 160/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0033\n",
            "Epoch 161/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0032\n",
            "Epoch 162/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0032\n",
            "Epoch 163/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0031\n",
            "Epoch 164/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0031\n",
            "Epoch 165/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0033\n",
            "Epoch 166/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0030\n",
            "Epoch 167/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0031\n",
            "Epoch 168/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0030\n",
            "Epoch 169/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0032\n",
            "Epoch 170/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0030\n",
            "Epoch 171/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0030\n",
            "Epoch 172/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0029\n",
            "Epoch 173/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0029\n",
            "Epoch 174/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0030\n",
            "Epoch 175/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0030\n",
            "Epoch 176/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0030\n",
            "Epoch 177/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0030\n",
            "Epoch 178/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0029\n",
            "Epoch 179/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0028\n",
            "Epoch 180/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0028\n",
            "Epoch 181/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0030\n",
            "Epoch 182/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0038\n",
            "Epoch 183/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0029\n",
            "Epoch 184/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0030\n",
            "Epoch 185/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0029\n",
            "Epoch 186/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0027\n",
            "Epoch 187/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0026\n",
            "Epoch 188/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0026\n",
            "Epoch 189/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0028\n",
            "Epoch 190/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0025\n",
            "Epoch 191/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0026\n",
            "Epoch 192/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0025\n",
            "Epoch 193/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0025\n",
            "Epoch 194/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0027\n",
            "Epoch 195/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0029\n",
            "Epoch 196/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0027\n",
            "Epoch 197/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
            "Epoch 198/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0024\n",
            "Epoch 199/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0025\n",
            "Epoch 200/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0024\n",
            "Epoch 201/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0025\n",
            "Epoch 202/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0026\n",
            "Epoch 203/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0025\n",
            "Epoch 204/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0024\n",
            "Epoch 205/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0024\n",
            "Epoch 206/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0025\n",
            "Epoch 207/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 208/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0024\n",
            "Epoch 209/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0024\n",
            "Epoch 210/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0025\n",
            "Epoch 211/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0023\n",
            "Epoch 212/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0025\n",
            "Epoch 213/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 214/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0023\n",
            "Epoch 215/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0025\n",
            "Epoch 216/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0025\n",
            "Epoch 217/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0025\n",
            "Epoch 218/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0022\n",
            "Epoch 219/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0024\n",
            "Epoch 220/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0023\n",
            "Epoch 221/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0024\n",
            "Epoch 222/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 223/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0024\n",
            "Epoch 224/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0022\n",
            "Epoch 225/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0022\n",
            "Epoch 226/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0022\n",
            "Epoch 227/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0022\n",
            "Epoch 228/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 229/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 230/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0020\n",
            "Epoch 231/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 232/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0022\n",
            "Epoch 233/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0022\n",
            "Epoch 234/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0023\n",
            "Epoch 235/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0024\n",
            "Epoch 236/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0021\n",
            "Epoch 237/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0022\n",
            "Epoch 238/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 239/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0021\n",
            "Epoch 240/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 241/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0020\n",
            "Epoch 242/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0020\n",
            "Epoch 243/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0023\n",
            "Epoch 244/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 245/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 246/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0020\n",
            "Epoch 247/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 248/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 249/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0019\n",
            "Epoch 250/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0019\n",
            "Epoch 251/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 252/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0020\n",
            "Epoch 253/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 254/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0018\n",
            "Epoch 255/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0018\n",
            "Epoch 256/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0021\n",
            "Epoch 257/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 258/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 259/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 260/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0019\n",
            "Epoch 261/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0018\n",
            "Epoch 262/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 263/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0017\n",
            "Epoch 264/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 265/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0017\n",
            "Epoch 266/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 267/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 268/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0018\n",
            "Epoch 269/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 270/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0019\n",
            "Epoch 271/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0019\n",
            "Epoch 272/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0016\n",
            "Epoch 273/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0016\n",
            "Epoch 274/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0018\n",
            "Epoch 275/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 276/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0017\n",
            "Epoch 277/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0019\n",
            "Epoch 278/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0019\n",
            "Epoch 279/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0016\n",
            "Epoch 280/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 281/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 282/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 283/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 284/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 285/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0018\n",
            "Epoch 286/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 287/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0017\n",
            "Epoch 288/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 289/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 290/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 291/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0014\n",
            "Epoch 292/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0018\n",
            "Epoch 293/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 294/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 295/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 296/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 297/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 298/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 299/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 300/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 301/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0013\n",
            "Epoch 302/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 303/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 304/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 305/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 306/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 307/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 308/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0015\n",
            "Epoch 309/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0015\n",
            "Epoch 310/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0013\n",
            "Epoch 311/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0015\n",
            "Epoch 312/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0014\n",
            "Epoch 313/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 314/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 315/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0014\n",
            "Epoch 316/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 317/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 318/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0014\n",
            "Epoch 319/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 320/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0014\n",
            "Epoch 321/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0015\n",
            "Epoch 322/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 323/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 324/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 325/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 326/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 327/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0012\n",
            "Epoch 328/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 329/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 330/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 331/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0011\n",
            "Epoch 332/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 333/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 334/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 335/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 336/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 337/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 338/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 339/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 340/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 341/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 342/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0012\n",
            "Epoch 343/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 344/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 345/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 346/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 347/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 348/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
            "Epoch 349/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 9.6646e-04\n",
            "Epoch 350/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 351/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0012\n",
            "Epoch 352/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0014\n",
            "Epoch 353/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
            "Epoch 354/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 355/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0010\n",
            "Epoch 356/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 9.7535e-04\n",
            "Epoch 357/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 9.8373e-04\n",
            "Epoch 358/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0011\n",
            "Epoch 359/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 360/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 361/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 362/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0013\n",
            "Epoch 363/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0011\n",
            "Epoch 364/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0010\n",
            "Epoch 365/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 366/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 367/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 0.0010\n",
            "Epoch 368/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 9.5505e-04\n",
            "Epoch 369/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0012\n",
            "Epoch 370/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.3907e-04\n",
            "Epoch 371/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 372/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 373/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0012\n",
            "Epoch 374/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 9.0319e-04\n",
            "Epoch 375/400\n",
            "39/39 - 0s - loss: 0.0031 - val_loss: 0.0012\n",
            "Epoch 376/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
            "Epoch 377/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0010\n",
            "Epoch 378/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 379/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 380/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 381/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 382/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 8.4069e-04\n",
            "Epoch 383/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 9.6858e-04\n",
            "Epoch 384/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 9.3228e-04\n",
            "Epoch 385/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 9.0069e-04\n",
            "Epoch 386/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
            "Epoch 387/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0019\n",
            "Epoch 388/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 9.5310e-04\n",
            "Epoch 389/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 390/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
            "Epoch 391/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 9.8902e-04\n",
            "Epoch 392/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 393/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0015\n",
            "Epoch 394/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.2351e-04\n",
            "Epoch 395/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 8.8775e-04\n",
            "Epoch 396/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 397/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 8.5288e-04\n",
            "Epoch 398/400\n",
            "39/39 - 0s - loss: 0.0029 - val_loss: 0.0012\n",
            "Epoch 399/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 8.9079e-04\n",
            "Epoch 400/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGRbfhpiYOlq",
        "outputId": "78c32b15-d65b-4fd9-a65a-a556efa9cc52"
      },
      "source": [
        "# baseline in performance with logistic regression model\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn import svm\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit model on training set\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# make prediction on test set\r\n",
        "yhat = model.predict(X_test)\r\n",
        "# calculate accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7662337662337663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VXN5bZYWXv4b",
        "outputId": "847642a2-ecda-474c-81e8-a866f363edeb"
      },
      "source": [
        "# train autoencoder for classification with with compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "# encoder level 1\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 2\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 3\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 4\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# bottleneck\r\n",
        "n_bottleneck = round(float(n_inputs) / 2.0)\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder, level 1\r\n",
        "d = Dense(n_inputs)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# decoder level 2\r\n",
        "d = Dense(n_inputs*2)(d)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder_compress.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 - 2s - loss: 0.6756 - val_loss: 0.1394\n",
            "Epoch 2/200\n",
            "44/44 - 0s - loss: 0.2862 - val_loss: 0.1072\n",
            "Epoch 3/200\n",
            "44/44 - 0s - loss: 0.1440 - val_loss: 0.0824\n",
            "Epoch 4/200\n",
            "44/44 - 0s - loss: 0.0805 - val_loss: 0.0635\n",
            "Epoch 5/200\n",
            "44/44 - 0s - loss: 0.0512 - val_loss: 0.0502\n",
            "Epoch 6/200\n",
            "44/44 - 0s - loss: 0.0384 - val_loss: 0.0404\n",
            "Epoch 7/200\n",
            "44/44 - 0s - loss: 0.0326 - val_loss: 0.0346\n",
            "Epoch 8/200\n",
            "44/44 - 0s - loss: 0.0293 - val_loss: 0.0302\n",
            "Epoch 9/200\n",
            "44/44 - 0s - loss: 0.0279 - val_loss: 0.0279\n",
            "Epoch 10/200\n",
            "44/44 - 0s - loss: 0.0273 - val_loss: 0.0279\n",
            "Epoch 11/200\n",
            "44/44 - 0s - loss: 0.0248 - val_loss: 0.0262\n",
            "Epoch 12/200\n",
            "44/44 - 0s - loss: 0.0238 - val_loss: 0.0242\n",
            "Epoch 13/200\n",
            "44/44 - 0s - loss: 0.0238 - val_loss: 0.0228\n",
            "Epoch 14/200\n",
            "44/44 - 0s - loss: 0.0230 - val_loss: 0.0226\n",
            "Epoch 15/200\n",
            "44/44 - 0s - loss: 0.0224 - val_loss: 0.0221\n",
            "Epoch 16/200\n",
            "44/44 - 0s - loss: 0.0215 - val_loss: 0.0223\n",
            "Epoch 17/200\n",
            "44/44 - 0s - loss: 0.0212 - val_loss: 0.0212\n",
            "Epoch 18/200\n",
            "44/44 - 0s - loss: 0.0212 - val_loss: 0.0210\n",
            "Epoch 19/200\n",
            "44/44 - 0s - loss: 0.0211 - val_loss: 0.0206\n",
            "Epoch 20/200\n",
            "44/44 - 0s - loss: 0.0209 - val_loss: 0.0200\n",
            "Epoch 21/200\n",
            "44/44 - 0s - loss: 0.0199 - val_loss: 0.0194\n",
            "Epoch 22/200\n",
            "44/44 - 0s - loss: 0.0198 - val_loss: 0.0189\n",
            "Epoch 23/200\n",
            "44/44 - 0s - loss: 0.0193 - val_loss: 0.0187\n",
            "Epoch 24/200\n",
            "44/44 - 0s - loss: 0.0190 - val_loss: 0.0187\n",
            "Epoch 25/200\n",
            "44/44 - 0s - loss: 0.0193 - val_loss: 0.0185\n",
            "Epoch 26/200\n",
            "44/44 - 0s - loss: 0.0193 - val_loss: 0.0193\n",
            "Epoch 27/200\n",
            "44/44 - 0s - loss: 0.0184 - val_loss: 0.0185\n",
            "Epoch 28/200\n",
            "44/44 - 0s - loss: 0.0186 - val_loss: 0.0182\n",
            "Epoch 29/200\n",
            "44/44 - 0s - loss: 0.0184 - val_loss: 0.0179\n",
            "Epoch 30/200\n",
            "44/44 - 0s - loss: 0.0176 - val_loss: 0.0181\n",
            "Epoch 31/200\n",
            "44/44 - 0s - loss: 0.0175 - val_loss: 0.0181\n",
            "Epoch 32/200\n",
            "44/44 - 0s - loss: 0.0181 - val_loss: 0.0177\n",
            "Epoch 33/200\n",
            "44/44 - 0s - loss: 0.0174 - val_loss: 0.0172\n",
            "Epoch 34/200\n",
            "44/44 - 0s - loss: 0.0176 - val_loss: 0.0171\n",
            "Epoch 35/200\n",
            "44/44 - 0s - loss: 0.0183 - val_loss: 0.0173\n",
            "Epoch 36/200\n",
            "44/44 - 0s - loss: 0.0178 - val_loss: 0.0171\n",
            "Epoch 37/200\n",
            "44/44 - 0s - loss: 0.0170 - val_loss: 0.0171\n",
            "Epoch 38/200\n",
            "44/44 - 0s - loss: 0.0172 - val_loss: 0.0169\n",
            "Epoch 39/200\n",
            "44/44 - 0s - loss: 0.0173 - val_loss: 0.0180\n",
            "Epoch 40/200\n",
            "44/44 - 0s - loss: 0.0168 - val_loss: 0.0170\n",
            "Epoch 41/200\n",
            "44/44 - 0s - loss: 0.0169 - val_loss: 0.0169\n",
            "Epoch 42/200\n",
            "44/44 - 0s - loss: 0.0166 - val_loss: 0.0167\n",
            "Epoch 43/200\n",
            "44/44 - 0s - loss: 0.0163 - val_loss: 0.0167\n",
            "Epoch 44/200\n",
            "44/44 - 0s - loss: 0.0162 - val_loss: 0.0163\n",
            "Epoch 45/200\n",
            "44/44 - 0s - loss: 0.0159 - val_loss: 0.0161\n",
            "Epoch 46/200\n",
            "44/44 - 0s - loss: 0.0161 - val_loss: 0.0160\n",
            "Epoch 47/200\n",
            "44/44 - 0s - loss: 0.0154 - val_loss: 0.0157\n",
            "Epoch 48/200\n",
            "44/44 - 0s - loss: 0.0155 - val_loss: 0.0158\n",
            "Epoch 49/200\n",
            "44/44 - 0s - loss: 0.0161 - val_loss: 0.0156\n",
            "Epoch 50/200\n",
            "44/44 - 0s - loss: 0.0155 - val_loss: 0.0154\n",
            "Epoch 51/200\n",
            "44/44 - 0s - loss: 0.0161 - val_loss: 0.0156\n",
            "Epoch 52/200\n",
            "44/44 - 0s - loss: 0.0164 - val_loss: 0.0151\n",
            "Epoch 53/200\n",
            "44/44 - 0s - loss: 0.0156 - val_loss: 0.0152\n",
            "Epoch 54/200\n",
            "44/44 - 0s - loss: 0.0156 - val_loss: 0.0149\n",
            "Epoch 55/200\n",
            "44/44 - 0s - loss: 0.0168 - val_loss: 0.0185\n",
            "Epoch 56/200\n",
            "44/44 - 0s - loss: 0.0158 - val_loss: 0.0164\n",
            "Epoch 57/200\n",
            "44/44 - 0s - loss: 0.0160 - val_loss: 0.0157\n",
            "Epoch 58/200\n",
            "44/44 - 0s - loss: 0.0152 - val_loss: 0.0151\n",
            "Epoch 59/200\n",
            "44/44 - 0s - loss: 0.0147 - val_loss: 0.0149\n",
            "Epoch 60/200\n",
            "44/44 - 0s - loss: 0.0147 - val_loss: 0.0147\n",
            "Epoch 61/200\n",
            "44/44 - 0s - loss: 0.0155 - val_loss: 0.0145\n",
            "Epoch 62/200\n",
            "44/44 - 0s - loss: 0.0149 - val_loss: 0.0148\n",
            "Epoch 63/200\n",
            "44/44 - 0s - loss: 0.0145 - val_loss: 0.0145\n",
            "Epoch 64/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0142\n",
            "Epoch 65/200\n",
            "44/44 - 0s - loss: 0.0148 - val_loss: 0.0143\n",
            "Epoch 66/200\n",
            "44/44 - 0s - loss: 0.0146 - val_loss: 0.0138\n",
            "Epoch 67/200\n",
            "44/44 - 0s - loss: 0.0142 - val_loss: 0.0136\n",
            "Epoch 68/200\n",
            "44/44 - 0s - loss: 0.0148 - val_loss: 0.0137\n",
            "Epoch 69/200\n",
            "44/44 - 0s - loss: 0.0143 - val_loss: 0.0137\n",
            "Epoch 70/200\n",
            "44/44 - 0s - loss: 0.0142 - val_loss: 0.0136\n",
            "Epoch 71/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0138\n",
            "Epoch 72/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0137\n",
            "Epoch 73/200\n",
            "44/44 - 0s - loss: 0.0141 - val_loss: 0.0135\n",
            "Epoch 74/200\n",
            "44/44 - 0s - loss: 0.0141 - val_loss: 0.0135\n",
            "Epoch 75/200\n",
            "44/44 - 0s - loss: 0.0142 - val_loss: 0.0133\n",
            "Epoch 76/200\n",
            "44/44 - 0s - loss: 0.0141 - val_loss: 0.0133\n",
            "Epoch 77/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0136\n",
            "Epoch 78/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0133\n",
            "Epoch 79/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0132\n",
            "Epoch 80/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0131\n",
            "Epoch 81/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0130\n",
            "Epoch 82/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0134\n",
            "Epoch 83/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0129\n",
            "Epoch 84/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0132\n",
            "Epoch 85/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0127\n",
            "Epoch 86/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0129\n",
            "Epoch 87/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0127\n",
            "Epoch 88/200\n",
            "44/44 - 0s - loss: 0.0139 - val_loss: 0.0129\n",
            "Epoch 89/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0139\n",
            "Epoch 90/200\n",
            "44/44 - 0s - loss: 0.0135 - val_loss: 0.0131\n",
            "Epoch 91/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0130\n",
            "Epoch 92/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0129\n",
            "Epoch 93/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0126\n",
            "Epoch 94/200\n",
            "44/44 - 0s - loss: 0.0135 - val_loss: 0.0126\n",
            "Epoch 95/200\n",
            "44/44 - 0s - loss: 0.0135 - val_loss: 0.0124\n",
            "Epoch 96/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0123\n",
            "Epoch 97/200\n",
            "44/44 - 0s - loss: 0.0131 - val_loss: 0.0123\n",
            "Epoch 98/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0123\n",
            "Epoch 99/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0121\n",
            "Epoch 100/200\n",
            "44/44 - 0s - loss: 0.0128 - val_loss: 0.0119\n",
            "Epoch 101/200\n",
            "44/44 - 0s - loss: 0.0133 - val_loss: 0.0118\n",
            "Epoch 102/200\n",
            "44/44 - 0s - loss: 0.0133 - val_loss: 0.0118\n",
            "Epoch 103/200\n",
            "44/44 - 0s - loss: 0.0131 - val_loss: 0.0119\n",
            "Epoch 104/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0117\n",
            "Epoch 105/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0117\n",
            "Epoch 106/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0117\n",
            "Epoch 107/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0118\n",
            "Epoch 108/200\n",
            "44/44 - 0s - loss: 0.0128 - val_loss: 0.0115\n",
            "Epoch 109/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0115\n",
            "Epoch 110/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0116\n",
            "Epoch 111/200\n",
            "44/44 - 0s - loss: 0.0121 - val_loss: 0.0114\n",
            "Epoch 112/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0115\n",
            "Epoch 113/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0113\n",
            "Epoch 114/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0115\n",
            "Epoch 115/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0113\n",
            "Epoch 116/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0112\n",
            "Epoch 117/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0111\n",
            "Epoch 118/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0110\n",
            "Epoch 119/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0110\n",
            "Epoch 120/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0109\n",
            "Epoch 121/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0110\n",
            "Epoch 122/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0110\n",
            "Epoch 123/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0109\n",
            "Epoch 124/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0108\n",
            "Epoch 125/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0109\n",
            "Epoch 126/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0110\n",
            "Epoch 127/200\n",
            "44/44 - 0s - loss: 0.0127 - val_loss: 0.0107\n",
            "Epoch 128/200\n",
            "44/44 - 0s - loss: 0.0119 - val_loss: 0.0106\n",
            "Epoch 129/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0105\n",
            "Epoch 130/200\n",
            "44/44 - 0s - loss: 0.0119 - val_loss: 0.0103\n",
            "Epoch 131/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0105\n",
            "Epoch 132/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0106\n",
            "Epoch 133/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0104\n",
            "Epoch 134/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0105\n",
            "Epoch 135/200\n",
            "44/44 - 0s - loss: 0.0120 - val_loss: 0.0104\n",
            "Epoch 136/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0104\n",
            "Epoch 137/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0106\n",
            "Epoch 138/200\n",
            "44/44 - 0s - loss: 0.0120 - val_loss: 0.0106\n",
            "Epoch 139/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0104\n",
            "Epoch 140/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0108\n",
            "Epoch 141/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0103\n",
            "Epoch 142/200\n",
            "44/44 - 0s - loss: 0.0121 - val_loss: 0.0105\n",
            "Epoch 143/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0106\n",
            "Epoch 144/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0106\n",
            "Epoch 145/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0104\n",
            "Epoch 146/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0103\n",
            "Epoch 147/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0102\n",
            "Epoch 148/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0103\n",
            "Epoch 149/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0101\n",
            "Epoch 150/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0102\n",
            "Epoch 151/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0102\n",
            "Epoch 152/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0103\n",
            "Epoch 153/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0105\n",
            "Epoch 154/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0102\n",
            "Epoch 155/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0105\n",
            "Epoch 156/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0101\n",
            "Epoch 157/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0103\n",
            "Epoch 158/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0102\n",
            "Epoch 159/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 160/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0103\n",
            "Epoch 161/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0102\n",
            "Epoch 162/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0102\n",
            "Epoch 163/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0101\n",
            "Epoch 164/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0101\n",
            "Epoch 165/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0099\n",
            "Epoch 166/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 167/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 168/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0101\n",
            "Epoch 169/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 170/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0100\n",
            "Epoch 171/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0097\n",
            "Epoch 172/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0097\n",
            "Epoch 173/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0100\n",
            "Epoch 174/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0097\n",
            "Epoch 175/200\n",
            "44/44 - 0s - loss: 0.0109 - val_loss: 0.0099\n",
            "Epoch 176/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0098\n",
            "Epoch 177/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0100\n",
            "Epoch 178/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0096\n",
            "Epoch 179/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0099\n",
            "Epoch 180/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0098\n",
            "Epoch 181/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0099\n",
            "Epoch 182/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0097\n",
            "Epoch 183/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0097\n",
            "Epoch 184/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0098\n",
            "Epoch 185/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0099\n",
            "Epoch 186/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0098\n",
            "Epoch 187/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0096\n",
            "Epoch 188/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0096\n",
            "Epoch 189/200\n",
            "44/44 - 0s - loss: 0.0109 - val_loss: 0.0095\n",
            "Epoch 190/200\n",
            "44/44 - 0s - loss: 0.0104 - val_loss: 0.0096\n",
            "Epoch 191/200\n",
            "44/44 - 0s - loss: 0.0109 - val_loss: 0.0095\n",
            "Epoch 192/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0095\n",
            "Epoch 193/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0096\n",
            "Epoch 194/200\n",
            "44/44 - 0s - loss: 0.0105 - val_loss: 0.0097\n",
            "Epoch 195/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0094\n",
            "Epoch 196/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0095\n",
            "Epoch 197/200\n",
            "44/44 - 0s - loss: 0.0109 - val_loss: 0.0095\n",
            "Epoch 198/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0092\n",
            "Epoch 199/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0092\n",
            "Epoch 200/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgN5VaavYhVx",
        "outputId": "d3ea3745-e325-4920-a473-52e47881701b"
      },
      "source": [
        "# evaluate logistic regression on encoded input\r\n",
        "import pandas\r\n",
        "from sklearn import model_selection\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from sklearn.ensemble import VotingClassifier\r\n",
        "from sklearn import svm\r\n",
        "# define dataset\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# load the model from file\r\n",
        "encoder = load_model('encoder.h5')\r\n",
        "# encode the train data\r\n",
        "X_train_encode = encoder.predict(X_train)\r\n",
        "# encode the test data\r\n",
        "X_test_encode = encoder.predict(X_test)\r\n",
        "# define the model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit the model on the training set\r\n",
        "model.fit(X_train_encode, y_train)\r\n",
        "# make predictions on the test set\r\n",
        "yhat = model.predict(X_test_encode)\r\n",
        "# calculate classification accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)\r\n",
        "\r\n",
        "model = svm.SVC()\r\n",
        "# fit the model on the training set\r\n",
        "model.fit(X_train_encode, y_train)\r\n",
        "# make predictions on the test set\r\n",
        "yhat = model.predict(X_test_encode)\r\n",
        "# calculate classification accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)\r\n",
        "\r\n",
        "clf = AdaBoostClassifier(n_estimators=100)\r\n",
        "scores = cross_val_score(clf, X, y, cv=5)\r\n",
        "print(scores.mean())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0.6948051948051948\n",
            "0.7727272727272727\n",
            "0.7617604617604619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Goz1R8lhhyE8",
        "outputId": "5ada8dee-02c5-46b3-aeaa-0f3396d021ef"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "\r\n",
        "clf = AdaBoostClassifier(n_estimators=100)\r\n",
        "scores = cross_val_score(clf, X, y, cv=5)\r\n",
        "scores.mean()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7617604617604619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}