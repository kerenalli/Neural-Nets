{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210203_Keras_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11MyEAm_I5TN1-M1cYlcbY3V7Bt6uWE15",
      "authorship_tag": "ABX9TyNOCcuX06BXaqBE3caDWvu2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerenalli/Neural-Nets/blob/main/20210203_Keras_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHqGmGP8SY3m",
        "outputId": "6047f0ac-442f-40ef-8487-53d75549f117"
      },
      "source": [
        "# first neural network with keras tutorial\r\n",
        "from numpy import loadtxt\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "# load the dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "# split into input (X) and output (y) variables\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# define the keras model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\r\n",
        "model.add(Dense(8, activation='relu'))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "# compile the keras model\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "# fit the keras model on the dataset\r\n",
        "model.fit(X, y, epochs=150, batch_size=10)\r\n",
        "# evaluate the keras model\r\n",
        "_, accuracy = model.evaluate(X, y)\r\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "77/77 [==============================] - 1s 1ms/step - loss: 13.7997 - accuracy: 0.5791\n",
            "Epoch 2/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 4.4510 - accuracy: 0.5034\n",
            "Epoch 3/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 2.3764 - accuracy: 0.5064\n",
            "Epoch 4/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 1.0329 - accuracy: 0.5895\n",
            "Epoch 5/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.9173 - accuracy: 0.5989\n",
            "Epoch 6/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.8459 - accuracy: 0.5960\n",
            "Epoch 7/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7941 - accuracy: 0.6224\n",
            "Epoch 8/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 1.0243 - accuracy: 0.5823\n",
            "Epoch 9/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7955 - accuracy: 0.6545\n",
            "Epoch 10/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7362 - accuracy: 0.6663\n",
            "Epoch 11/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7156 - accuracy: 0.6342\n",
            "Epoch 12/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.8413 - accuracy: 0.6091\n",
            "Epoch 13/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.6683\n",
            "Epoch 14/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7364 - accuracy: 0.6607\n",
            "Epoch 15/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7293 - accuracy: 0.6486\n",
            "Epoch 16/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6432 - accuracy: 0.6917\n",
            "Epoch 17/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6612 - accuracy: 0.6779\n",
            "Epoch 18/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6635 - accuracy: 0.6839\n",
            "Epoch 19/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6031 - accuracy: 0.7039\n",
            "Epoch 20/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6092 - accuracy: 0.6999\n",
            "Epoch 21/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.8220 - accuracy: 0.6581\n",
            "Epoch 22/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6518 - accuracy: 0.6746\n",
            "Epoch 23/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7297 - accuracy: 0.6676\n",
            "Epoch 24/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7002 - accuracy: 0.6489\n",
            "Epoch 25/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6099 - accuracy: 0.7039\n",
            "Epoch 26/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6205 - accuracy: 0.6732\n",
            "Epoch 27/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.8387 - accuracy: 0.6279\n",
            "Epoch 28/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5743 - accuracy: 0.7172\n",
            "Epoch 29/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6201 - accuracy: 0.6782\n",
            "Epoch 30/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5810 - accuracy: 0.7005\n",
            "Epoch 31/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6070 - accuracy: 0.7006\n",
            "Epoch 32/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5849 - accuracy: 0.7038\n",
            "Epoch 33/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6164 - accuracy: 0.7121\n",
            "Epoch 34/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7263 - accuracy: 0.6285\n",
            "Epoch 35/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6251 - accuracy: 0.6645\n",
            "Epoch 36/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6455 - accuracy: 0.7006\n",
            "Epoch 37/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6039 - accuracy: 0.6980\n",
            "Epoch 38/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6238 - accuracy: 0.7002\n",
            "Epoch 39/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6746 - accuracy: 0.6866\n",
            "Epoch 40/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5377 - accuracy: 0.7523\n",
            "Epoch 41/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6400 - accuracy: 0.7069\n",
            "Epoch 42/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6087 - accuracy: 0.7041\n",
            "Epoch 43/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6247 - accuracy: 0.7049\n",
            "Epoch 44/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5822 - accuracy: 0.7290\n",
            "Epoch 45/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6846 - accuracy: 0.6810\n",
            "Epoch 46/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7156\n",
            "Epoch 47/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.7228\n",
            "Epoch 48/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7262\n",
            "Epoch 49/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6196 - accuracy: 0.6883\n",
            "Epoch 50/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5527 - accuracy: 0.7414\n",
            "Epoch 51/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.7106\n",
            "Epoch 52/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5865 - accuracy: 0.7066\n",
            "Epoch 53/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5359 - accuracy: 0.7634\n",
            "Epoch 54/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7195\n",
            "Epoch 55/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.7214\n",
            "Epoch 56/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.7075\n",
            "Epoch 57/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5483 - accuracy: 0.7231\n",
            "Epoch 58/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.7541\n",
            "Epoch 59/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7534\n",
            "Epoch 60/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.7211\n",
            "Epoch 61/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.7357\n",
            "Epoch 62/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5383 - accuracy: 0.7347\n",
            "Epoch 63/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5266 - accuracy: 0.7537\n",
            "Epoch 64/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5428 - accuracy: 0.7611\n",
            "Epoch 65/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5657 - accuracy: 0.7290\n",
            "Epoch 66/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5171 - accuracy: 0.7515\n",
            "Epoch 67/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6808\n",
            "Epoch 68/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5829 - accuracy: 0.7030\n",
            "Epoch 69/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5843 - accuracy: 0.7347\n",
            "Epoch 70/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 0.7128\n",
            "Epoch 71/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7527\n",
            "Epoch 72/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.7647\n",
            "Epoch 73/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7541\n",
            "Epoch 74/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6377 - accuracy: 0.6902\n",
            "Epoch 75/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7491\n",
            "Epoch 76/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.7308\n",
            "Epoch 77/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5017 - accuracy: 0.7631\n",
            "Epoch 78/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5447 - accuracy: 0.7170\n",
            "Epoch 79/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7608\n",
            "Epoch 80/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7807\n",
            "Epoch 81/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5780 - accuracy: 0.7214\n",
            "Epoch 82/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.7104\n",
            "Epoch 83/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.7145\n",
            "Epoch 84/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7434\n",
            "Epoch 85/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7558\n",
            "Epoch 86/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5220 - accuracy: 0.7383\n",
            "Epoch 87/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7517\n",
            "Epoch 88/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7446\n",
            "Epoch 89/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.7355\n",
            "Epoch 90/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5947 - accuracy: 0.6960\n",
            "Epoch 91/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7469\n",
            "Epoch 92/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7633\n",
            "Epoch 93/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5309 - accuracy: 0.7260\n",
            "Epoch 94/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7524\n",
            "Epoch 95/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7665\n",
            "Epoch 96/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4995 - accuracy: 0.7693\n",
            "Epoch 97/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5244 - accuracy: 0.7375\n",
            "Epoch 98/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7459\n",
            "Epoch 99/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.7079\n",
            "Epoch 100/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.7729\n",
            "Epoch 101/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7315\n",
            "Epoch 102/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5227 - accuracy: 0.7587\n",
            "Epoch 103/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5519 - accuracy: 0.7335\n",
            "Epoch 104/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7494\n",
            "Epoch 105/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.7608\n",
            "Epoch 106/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4860 - accuracy: 0.7666\n",
            "Epoch 107/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7473\n",
            "Epoch 108/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7556\n",
            "Epoch 109/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7669\n",
            "Epoch 110/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7470\n",
            "Epoch 111/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.6919\n",
            "Epoch 112/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7546\n",
            "Epoch 113/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.7358\n",
            "Epoch 114/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7459\n",
            "Epoch 115/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.7604\n",
            "Epoch 116/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5042 - accuracy: 0.7616\n",
            "Epoch 117/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5611 - accuracy: 0.7273\n",
            "Epoch 118/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7534\n",
            "Epoch 119/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5491 - accuracy: 0.7525\n",
            "Epoch 120/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4971 - accuracy: 0.7765\n",
            "Epoch 121/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7716\n",
            "Epoch 122/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.7621\n",
            "Epoch 123/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5951 - accuracy: 0.7463\n",
            "Epoch 124/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5201 - accuracy: 0.7726\n",
            "Epoch 125/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7403\n",
            "Epoch 126/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.7772\n",
            "Epoch 127/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4965 - accuracy: 0.7636\n",
            "Epoch 128/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.7540\n",
            "Epoch 129/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.7596\n",
            "Epoch 130/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5372 - accuracy: 0.7299\n",
            "Epoch 131/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.7518\n",
            "Epoch 132/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.8054\n",
            "Epoch 133/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7529\n",
            "Epoch 134/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.7064\n",
            "Epoch 135/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7924\n",
            "Epoch 136/150\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4829 - accuracy: 0.7670\n",
            "Epoch 137/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7958\n",
            "Epoch 138/150\n",
            "77/77 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.7824\n",
            "Epoch 139/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7465\n",
            "Epoch 140/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7546\n",
            "Epoch 141/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7368\n",
            "Epoch 142/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7696\n",
            "Epoch 143/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7654\n",
            "Epoch 144/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7555\n",
            "Epoch 145/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7632\n",
            "Epoch 146/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7498\n",
            "Epoch 147/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7961\n",
            "Epoch 148/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7628\n",
            "Epoch 149/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7871\n",
            "Epoch 150/150\n",
            "77/77 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.7330\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7708\n",
            "Accuracy: 77.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "25IaTsMQNvDG",
        "outputId": "d036959d-0bc9-4a15-cbb5-390e5b73717c"
      },
      "source": [
        "# train autoencoder for regression with no compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_regression\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import ReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = ReLU()(e)\r\n",
        "# define bottleneck\r\n",
        "n_bottleneck = n_inputs\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder\r\n",
        "d = Dense(n_inputs*2)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = ReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=400, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "39/39 - 1s - loss: 0.3572 - val_loss: 0.1517\n",
            "Epoch 2/400\n",
            "39/39 - 0s - loss: 0.1552 - val_loss: 0.1136\n",
            "Epoch 3/400\n",
            "39/39 - 0s - loss: 0.0862 - val_loss: 0.0854\n",
            "Epoch 4/400\n",
            "39/39 - 0s - loss: 0.0623 - val_loss: 0.0660\n",
            "Epoch 5/400\n",
            "39/39 - 0s - loss: 0.0476 - val_loss: 0.0517\n",
            "Epoch 6/400\n",
            "39/39 - 0s - loss: 0.0403 - val_loss: 0.0409\n",
            "Epoch 7/400\n",
            "39/39 - 0s - loss: 0.0345 - val_loss: 0.0334\n",
            "Epoch 8/400\n",
            "39/39 - 0s - loss: 0.0309 - val_loss: 0.0283\n",
            "Epoch 9/400\n",
            "39/39 - 0s - loss: 0.0277 - val_loss: 0.0236\n",
            "Epoch 10/400\n",
            "39/39 - 0s - loss: 0.0245 - val_loss: 0.0199\n",
            "Epoch 11/400\n",
            "39/39 - 0s - loss: 0.0222 - val_loss: 0.0178\n",
            "Epoch 12/400\n",
            "39/39 - 0s - loss: 0.0207 - val_loss: 0.0158\n",
            "Epoch 13/400\n",
            "39/39 - 0s - loss: 0.0207 - val_loss: 0.0147\n",
            "Epoch 14/400\n",
            "39/39 - 0s - loss: 0.0177 - val_loss: 0.0142\n",
            "Epoch 15/400\n",
            "39/39 - 0s - loss: 0.0174 - val_loss: 0.0135\n",
            "Epoch 16/400\n",
            "39/39 - 0s - loss: 0.0161 - val_loss: 0.0129\n",
            "Epoch 17/400\n",
            "39/39 - 0s - loss: 0.0163 - val_loss: 0.0128\n",
            "Epoch 18/400\n",
            "39/39 - 0s - loss: 0.0156 - val_loss: 0.0122\n",
            "Epoch 19/400\n",
            "39/39 - 0s - loss: 0.0151 - val_loss: 0.0119\n",
            "Epoch 20/400\n",
            "39/39 - 0s - loss: 0.0146 - val_loss: 0.0114\n",
            "Epoch 21/400\n",
            "39/39 - 0s - loss: 0.0147 - val_loss: 0.0108\n",
            "Epoch 22/400\n",
            "39/39 - 0s - loss: 0.0134 - val_loss: 0.0104\n",
            "Epoch 23/400\n",
            "39/39 - 0s - loss: 0.0130 - val_loss: 0.0099\n",
            "Epoch 24/400\n",
            "39/39 - 0s - loss: 0.0123 - val_loss: 0.0095\n",
            "Epoch 25/400\n",
            "39/39 - 0s - loss: 0.0122 - val_loss: 0.0094\n",
            "Epoch 26/400\n",
            "39/39 - 0s - loss: 0.0121 - val_loss: 0.0093\n",
            "Epoch 27/400\n",
            "39/39 - 0s - loss: 0.0120 - val_loss: 0.0089\n",
            "Epoch 28/400\n",
            "39/39 - 0s - loss: 0.0124 - val_loss: 0.0088\n",
            "Epoch 29/400\n",
            "39/39 - 0s - loss: 0.0118 - val_loss: 0.0087\n",
            "Epoch 30/400\n",
            "39/39 - 0s - loss: 0.0112 - val_loss: 0.0086\n",
            "Epoch 31/400\n",
            "39/39 - 0s - loss: 0.0117 - val_loss: 0.0085\n",
            "Epoch 32/400\n",
            "39/39 - 0s - loss: 0.0112 - val_loss: 0.0081\n",
            "Epoch 33/400\n",
            "39/39 - 0s - loss: 0.0109 - val_loss: 0.0081\n",
            "Epoch 34/400\n",
            "39/39 - 0s - loss: 0.0108 - val_loss: 0.0079\n",
            "Epoch 35/400\n",
            "39/39 - 0s - loss: 0.0108 - val_loss: 0.0078\n",
            "Epoch 36/400\n",
            "39/39 - 0s - loss: 0.0105 - val_loss: 0.0077\n",
            "Epoch 37/400\n",
            "39/39 - 0s - loss: 0.0104 - val_loss: 0.0074\n",
            "Epoch 38/400\n",
            "39/39 - 0s - loss: 0.0102 - val_loss: 0.0075\n",
            "Epoch 39/400\n",
            "39/39 - 0s - loss: 0.0101 - val_loss: 0.0072\n",
            "Epoch 40/400\n",
            "39/39 - 0s - loss: 0.0102 - val_loss: 0.0071\n",
            "Epoch 41/400\n",
            "39/39 - 0s - loss: 0.0105 - val_loss: 0.0071\n",
            "Epoch 42/400\n",
            "39/39 - 0s - loss: 0.0095 - val_loss: 0.0071\n",
            "Epoch 43/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0070\n",
            "Epoch 44/400\n",
            "39/39 - 0s - loss: 0.0093 - val_loss: 0.0068\n",
            "Epoch 45/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0067\n",
            "Epoch 46/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0066\n",
            "Epoch 47/400\n",
            "39/39 - 0s - loss: 0.0098 - val_loss: 0.0066\n",
            "Epoch 48/400\n",
            "39/39 - 0s - loss: 0.0093 - val_loss: 0.0063\n",
            "Epoch 49/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0066\n",
            "Epoch 50/400\n",
            "39/39 - 0s - loss: 0.0096 - val_loss: 0.0063\n",
            "Epoch 51/400\n",
            "39/39 - 0s - loss: 0.0091 - val_loss: 0.0063\n",
            "Epoch 52/400\n",
            "39/39 - 0s - loss: 0.0087 - val_loss: 0.0062\n",
            "Epoch 53/400\n",
            "39/39 - 0s - loss: 0.0087 - val_loss: 0.0061\n",
            "Epoch 54/400\n",
            "39/39 - 0s - loss: 0.0087 - val_loss: 0.0061\n",
            "Epoch 55/400\n",
            "39/39 - 0s - loss: 0.0088 - val_loss: 0.0060\n",
            "Epoch 56/400\n",
            "39/39 - 0s - loss: 0.0089 - val_loss: 0.0062\n",
            "Epoch 57/400\n",
            "39/39 - 0s - loss: 0.0080 - val_loss: 0.0060\n",
            "Epoch 58/400\n",
            "39/39 - 0s - loss: 0.0087 - val_loss: 0.0057\n",
            "Epoch 59/400\n",
            "39/39 - 0s - loss: 0.0085 - val_loss: 0.0058\n",
            "Epoch 60/400\n",
            "39/39 - 0s - loss: 0.0085 - val_loss: 0.0058\n",
            "Epoch 61/400\n",
            "39/39 - 0s - loss: 0.0082 - val_loss: 0.0056\n",
            "Epoch 62/400\n",
            "39/39 - 0s - loss: 0.0083 - val_loss: 0.0056\n",
            "Epoch 63/400\n",
            "39/39 - 0s - loss: 0.0081 - val_loss: 0.0055\n",
            "Epoch 64/400\n",
            "39/39 - 0s - loss: 0.0077 - val_loss: 0.0055\n",
            "Epoch 65/400\n",
            "39/39 - 0s - loss: 0.0076 - val_loss: 0.0054\n",
            "Epoch 66/400\n",
            "39/39 - 0s - loss: 0.0083 - val_loss: 0.0054\n",
            "Epoch 67/400\n",
            "39/39 - 0s - loss: 0.0082 - val_loss: 0.0055\n",
            "Epoch 68/400\n",
            "39/39 - 0s - loss: 0.0080 - val_loss: 0.0054\n",
            "Epoch 69/400\n",
            "39/39 - 0s - loss: 0.0082 - val_loss: 0.0052\n",
            "Epoch 70/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0053\n",
            "Epoch 71/400\n",
            "39/39 - 0s - loss: 0.0072 - val_loss: 0.0051\n",
            "Epoch 72/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0051\n",
            "Epoch 73/400\n",
            "39/39 - 0s - loss: 0.0076 - val_loss: 0.0049\n",
            "Epoch 74/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0049\n",
            "Epoch 75/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0049\n",
            "Epoch 76/400\n",
            "39/39 - 0s - loss: 0.0075 - val_loss: 0.0048\n",
            "Epoch 77/400\n",
            "39/39 - 0s - loss: 0.0078 - val_loss: 0.0049\n",
            "Epoch 78/400\n",
            "39/39 - 0s - loss: 0.0070 - val_loss: 0.0046\n",
            "Epoch 79/400\n",
            "39/39 - 0s - loss: 0.0070 - val_loss: 0.0047\n",
            "Epoch 80/400\n",
            "39/39 - 0s - loss: 0.0069 - val_loss: 0.0046\n",
            "Epoch 81/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0047\n",
            "Epoch 82/400\n",
            "39/39 - 0s - loss: 0.0073 - val_loss: 0.0045\n",
            "Epoch 83/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0046\n",
            "Epoch 84/400\n",
            "39/39 - 0s - loss: 0.0074 - val_loss: 0.0045\n",
            "Epoch 85/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0043\n",
            "Epoch 86/400\n",
            "39/39 - 0s - loss: 0.0071 - val_loss: 0.0042\n",
            "Epoch 87/400\n",
            "39/39 - 0s - loss: 0.0066 - val_loss: 0.0042\n",
            "Epoch 88/400\n",
            "39/39 - 0s - loss: 0.0071 - val_loss: 0.0043\n",
            "Epoch 89/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0044\n",
            "Epoch 90/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0041\n",
            "Epoch 91/400\n",
            "39/39 - 0s - loss: 0.0071 - val_loss: 0.0042\n",
            "Epoch 92/400\n",
            "39/39 - 0s - loss: 0.0065 - val_loss: 0.0041\n",
            "Epoch 93/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0040\n",
            "Epoch 94/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0039\n",
            "Epoch 95/400\n",
            "39/39 - 0s - loss: 0.0065 - val_loss: 0.0039\n",
            "Epoch 96/400\n",
            "39/39 - 0s - loss: 0.0068 - val_loss: 0.0038\n",
            "Epoch 97/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0038\n",
            "Epoch 98/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0037\n",
            "Epoch 99/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0037\n",
            "Epoch 100/400\n",
            "39/39 - 0s - loss: 0.0064 - val_loss: 0.0037\n",
            "Epoch 101/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0036\n",
            "Epoch 102/400\n",
            "39/39 - 0s - loss: 0.0067 - val_loss: 0.0039\n",
            "Epoch 103/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0036\n",
            "Epoch 104/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0035\n",
            "Epoch 105/400\n",
            "39/39 - 0s - loss: 0.0063 - val_loss: 0.0035\n",
            "Epoch 106/400\n",
            "39/39 - 0s - loss: 0.0061 - val_loss: 0.0033\n",
            "Epoch 107/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0034\n",
            "Epoch 108/400\n",
            "39/39 - 0s - loss: 0.0062 - val_loss: 0.0035\n",
            "Epoch 109/400\n",
            "39/39 - 0s - loss: 0.0058 - val_loss: 0.0033\n",
            "Epoch 110/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0033\n",
            "Epoch 111/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0033\n",
            "Epoch 112/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0031\n",
            "Epoch 113/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0031\n",
            "Epoch 114/400\n",
            "39/39 - 0s - loss: 0.0059 - val_loss: 0.0034\n",
            "Epoch 115/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0031\n",
            "Epoch 116/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0031\n",
            "Epoch 117/400\n",
            "39/39 - 0s - loss: 0.0060 - val_loss: 0.0031\n",
            "Epoch 118/400\n",
            "39/39 - 0s - loss: 0.0056 - val_loss: 0.0030\n",
            "Epoch 119/400\n",
            "39/39 - 0s - loss: 0.0066 - val_loss: 0.0030\n",
            "Epoch 120/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0030\n",
            "Epoch 121/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0027\n",
            "Epoch 122/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0029\n",
            "Epoch 123/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0027\n",
            "Epoch 124/400\n",
            "39/39 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
            "Epoch 125/400\n",
            "39/39 - 0s - loss: 0.0057 - val_loss: 0.0026\n",
            "Epoch 126/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0026\n",
            "Epoch 127/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0027\n",
            "Epoch 128/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0025\n",
            "Epoch 129/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0025\n",
            "Epoch 130/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0026\n",
            "Epoch 131/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0025\n",
            "Epoch 132/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0025\n",
            "Epoch 133/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0025\n",
            "Epoch 134/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0023\n",
            "Epoch 135/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0024\n",
            "Epoch 136/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0023\n",
            "Epoch 137/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0022\n",
            "Epoch 138/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0023\n",
            "Epoch 139/400\n",
            "39/39 - 0s - loss: 0.0053 - val_loss: 0.0021\n",
            "Epoch 140/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0024\n",
            "Epoch 141/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 142/400\n",
            "39/39 - 0s - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 143/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0022\n",
            "Epoch 144/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0021\n",
            "Epoch 145/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0019\n",
            "Epoch 146/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0020\n",
            "Epoch 147/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0019\n",
            "Epoch 148/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 149/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0019\n",
            "Epoch 150/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0020\n",
            "Epoch 151/400\n",
            "39/39 - 0s - loss: 0.0051 - val_loss: 0.0019\n",
            "Epoch 152/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0019\n",
            "Epoch 153/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0020\n",
            "Epoch 154/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0019\n",
            "Epoch 155/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0018\n",
            "Epoch 156/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 157/400\n",
            "39/39 - 0s - loss: 0.0049 - val_loss: 0.0018\n",
            "Epoch 158/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0018\n",
            "Epoch 159/400\n",
            "39/39 - 0s - loss: 0.0054 - val_loss: 0.0019\n",
            "Epoch 160/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 161/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0019\n",
            "Epoch 162/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 163/400\n",
            "39/39 - 0s - loss: 0.0050 - val_loss: 0.0017\n",
            "Epoch 164/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0017\n",
            "Epoch 165/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0020\n",
            "Epoch 166/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 167/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 168/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0018\n",
            "Epoch 169/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 170/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0018\n",
            "Epoch 171/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 172/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0015\n",
            "Epoch 173/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 174/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0017\n",
            "Epoch 175/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0018\n",
            "Epoch 176/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 177/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0018\n",
            "Epoch 178/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0016\n",
            "Epoch 179/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 180/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 181/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 182/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0015\n",
            "Epoch 183/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 184/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0013\n",
            "Epoch 185/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0014\n",
            "Epoch 186/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 187/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0016\n",
            "Epoch 188/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0013\n",
            "Epoch 189/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0014\n",
            "Epoch 190/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 191/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0014\n",
            "Epoch 192/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 193/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 194/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 195/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0015\n",
            "Epoch 196/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 197/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0015\n",
            "Epoch 198/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0016\n",
            "Epoch 199/400\n",
            "39/39 - 0s - loss: 0.0048 - val_loss: 0.0013\n",
            "Epoch 200/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0013\n",
            "Epoch 201/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0016\n",
            "Epoch 202/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 203/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0014\n",
            "Epoch 204/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 205/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 206/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 207/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 208/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0013\n",
            "Epoch 209/400\n",
            "39/39 - 0s - loss: 0.0047 - val_loss: 0.0012\n",
            "Epoch 210/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 211/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 212/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 213/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0013\n",
            "Epoch 214/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 215/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0014\n",
            "Epoch 216/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 217/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 218/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 219/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0012\n",
            "Epoch 220/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0011\n",
            "Epoch 221/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 222/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 223/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 224/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 225/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 226/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0011\n",
            "Epoch 227/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 228/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 229/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 230/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 231/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 232/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 233/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0014\n",
            "Epoch 234/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 235/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 236/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0013\n",
            "Epoch 237/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 238/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 239/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0014\n",
            "Epoch 240/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0011\n",
            "Epoch 241/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0014\n",
            "Epoch 242/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 9.6393e-04\n",
            "Epoch 243/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0011\n",
            "Epoch 244/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 245/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 246/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 247/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 248/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 249/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.8960e-04\n",
            "Epoch 250/400\n",
            "39/39 - 0s - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 251/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 252/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 253/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0010\n",
            "Epoch 254/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 255/400\n",
            "39/39 - 0s - loss: 0.0045 - val_loss: 0.0016\n",
            "Epoch 256/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 9.9247e-04\n",
            "Epoch 257/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 258/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0011\n",
            "Epoch 259/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0011\n",
            "Epoch 260/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 261/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0010\n",
            "Epoch 262/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.4574e-04\n",
            "Epoch 263/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0010\n",
            "Epoch 264/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0012\n",
            "Epoch 265/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 9.8533e-04\n",
            "Epoch 266/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0012\n",
            "Epoch 267/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 9.5867e-04\n",
            "Epoch 268/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0010\n",
            "Epoch 269/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0010\n",
            "Epoch 270/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 0.0014\n",
            "Epoch 271/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 272/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 273/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0010\n",
            "Epoch 274/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0010\n",
            "Epoch 275/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0012\n",
            "Epoch 276/400\n",
            "39/39 - 0s - loss: 0.0046 - val_loss: 0.0013\n",
            "Epoch 277/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.5909e-04\n",
            "Epoch 278/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 279/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 280/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.6961e-04\n",
            "Epoch 281/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0010\n",
            "Epoch 282/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0013\n",
            "Epoch 283/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 9.3977e-04\n",
            "Epoch 284/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 285/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0011\n",
            "Epoch 286/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 287/400\n",
            "39/39 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 288/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 289/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0016\n",
            "Epoch 290/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 291/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 292/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 293/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0010\n",
            "Epoch 294/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 9.6461e-04\n",
            "Epoch 295/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0011\n",
            "Epoch 296/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 297/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0010\n",
            "Epoch 298/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 299/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 300/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0011\n",
            "Epoch 301/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 302/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.8667e-04\n",
            "Epoch 303/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0014\n",
            "Epoch 304/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 305/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0011\n",
            "Epoch 306/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 8.7047e-04\n",
            "Epoch 307/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 308/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.0410e-04\n",
            "Epoch 309/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0012\n",
            "Epoch 310/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0013\n",
            "Epoch 311/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 312/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 313/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.5201e-04\n",
            "Epoch 314/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 315/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 0.0010\n",
            "Epoch 316/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 317/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 8.3923e-04\n",
            "Epoch 318/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0011\n",
            "Epoch 319/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 320/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 9.8033e-04\n",
            "Epoch 321/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 9.9654e-04\n",
            "Epoch 322/400\n",
            "39/39 - 0s - loss: 0.0042 - val_loss: 8.8086e-04\n",
            "Epoch 323/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0014\n",
            "Epoch 324/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0021\n",
            "Epoch 325/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.9334e-04\n",
            "Epoch 326/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0010\n",
            "Epoch 327/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 9.9889e-04\n",
            "Epoch 328/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0022\n",
            "Epoch 329/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 7.6516e-04\n",
            "Epoch 330/400\n",
            "39/39 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 331/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0010\n",
            "Epoch 332/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0013\n",
            "Epoch 333/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 334/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 335/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0011\n",
            "Epoch 336/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0011\n",
            "Epoch 337/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
            "Epoch 338/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 9.7286e-04\n",
            "Epoch 339/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0010\n",
            "Epoch 340/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 341/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 342/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 343/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 344/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 7.8840e-04\n",
            "Epoch 345/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 8.4616e-04\n",
            "Epoch 346/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 8.7548e-04\n",
            "Epoch 347/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0010\n",
            "Epoch 348/400\n",
            "39/39 - 0s - loss: 0.0040 - val_loss: 9.5003e-04\n",
            "Epoch 349/400\n",
            "39/39 - 0s - loss: 0.0041 - val_loss: 0.0014\n",
            "Epoch 350/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 7.8984e-04\n",
            "Epoch 351/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 352/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 353/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 354/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 7.6920e-04\n",
            "Epoch 355/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 9.0503e-04\n",
            "Epoch 356/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 9.5019e-04\n",
            "Epoch 357/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 7.1275e-04\n",
            "Epoch 358/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 359/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 6.7943e-04\n",
            "Epoch 360/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 7.7267e-04\n",
            "Epoch 361/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 362/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 7.3723e-04\n",
            "Epoch 363/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 6.9825e-04\n",
            "Epoch 364/400\n",
            "39/39 - 0s - loss: 0.0037 - val_loss: 0.0011\n",
            "Epoch 365/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 6.8103e-04\n",
            "Epoch 366/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 8.3861e-04\n",
            "Epoch 367/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 6.2949e-04\n",
            "Epoch 368/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 369/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 6.8768e-04\n",
            "Epoch 370/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 9.6410e-04\n",
            "Epoch 371/400\n",
            "39/39 - 0s - loss: 0.0038 - val_loss: 0.0011\n",
            "Epoch 372/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 373/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 8.2716e-04\n",
            "Epoch 374/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.7695e-04\n",
            "Epoch 375/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 9.1861e-04\n",
            "Epoch 376/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 8.0809e-04\n",
            "Epoch 377/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 0.0010\n",
            "Epoch 378/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 9.0740e-04\n",
            "Epoch 379/400\n",
            "39/39 - 0s - loss: 0.0039 - val_loss: 8.7645e-04\n",
            "Epoch 380/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 7.2387e-04\n",
            "Epoch 381/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 6.8950e-04\n",
            "Epoch 382/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 7.7865e-04\n",
            "Epoch 383/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.7799e-04\n",
            "Epoch 384/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 6.9629e-04\n",
            "Epoch 385/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 8.5711e-04\n",
            "Epoch 386/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 8.0471e-04\n",
            "Epoch 387/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0011\n",
            "Epoch 388/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 6.7243e-04\n",
            "Epoch 389/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 5.5204e-04\n",
            "Epoch 390/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 7.7861e-04\n",
            "Epoch 391/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.8491e-04\n",
            "Epoch 392/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 9.5286e-04\n",
            "Epoch 393/400\n",
            "39/39 - 0s - loss: 0.0035 - val_loss: 7.8249e-04\n",
            "Epoch 394/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 7.5219e-04\n",
            "Epoch 395/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 0.0010\n",
            "Epoch 396/400\n",
            "39/39 - 0s - loss: 0.0036 - val_loss: 9.5311e-04\n",
            "Epoch 397/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 9.6978e-04\n",
            "Epoch 398/400\n",
            "39/39 - 0s - loss: 0.0033 - val_loss: 7.2488e-04\n",
            "Epoch 399/400\n",
            "39/39 - 0s - loss: 0.0034 - val_loss: 0.0011\n",
            "Epoch 400/400\n",
            "39/39 - 0s - loss: 0.0032 - val_loss: 8.5066e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZCcB3nn8e/Td899SpY0umzLxMaObTwoJiaEyyDDxjYVDsO64lRRpZDgCrspWOwKMcHZ1BKyyxK2zGGCNteCIZBDSeSyDbYhu2CQfIAl2bIObGskWRqNNPdMn8/+8b4jtXpmpJY0Mz1+9ftUTU33e/T79KvR7337ed9+X3N3REQkumL1LkBEROaXgl5EJOIU9CIiEaegFxGJOAW9iEjEJepdQLWuri5fs2ZNvcsQEXlVefLJJ4+6e/dM4xZd0K9Zs4Zt27bVuwwRkVcVM3tptnFq3YiIRJyCXkQk4hT0IiIRt+h69CIi56JQKNDX18fk5GS9S5lXmUyGnp4ekslkzfMo6EUkEvr6+mhubmbNmjWYWb3LmRfuzsDAAH19faxdu7bm+dS6EZFImJycpLOzM7IhD2BmdHZ2nvWnFgW9iERGlEN+yrm8x8gE/ViuyOcf3sXTLx+vdykiIotKZIJ+slDii4/u4dkDQ/UuRUQuQIODg3zpS1866/ne9a53MTg4OA8VnVRT0JvZBjPbZWZ7zOyuGcZ/xMyeNbNnzOz/mtkV4fA1ZjYRDn/GzL4y129gSiz8OFMu60YqIrLwZgv6YrF42vm2bNlCW1vbfJUF1HDWjZnFgfuAG4E+YKuZbXb3nRWTfcPdvxJOfzPweWBDOG6vu18zt2VPdyLolfMiUgd33XUXe/fu5ZprriGZTJLJZGhvb+f555/nhRde4NZbb2X//v1MTk7ysY99jI0bNwInL/syOjrKTTfdxBvf+EZ+9KMfsWLFCv75n/+ZbDZ73rXVcnrlemCPu+8DMLMHgFuAE0Hv7sMV0zcCCx+34fGJsm6NKHLB+8y/7GDnweEzT3gWrljewqd/47Wzjv/sZz/L9u3beeaZZ3j88cd597vfzfbt20+cBrlp0yY6OjqYmJjg9a9/Pb/5m79JZ2fnKa+xe/duvvnNb/K1r32N97///Xz3u9/l9ttvP+/aa2ndrAD2VzzvC4edwsw+amZ7gc8Bv18xaq2ZPW1mPzCzX5tpAWa20cy2mdm2/v7+syj/pFj0D7aLyKvI+vXrTznX/Ytf/CJXX301119/Pfv372f37t3T5lm7di3XXBM0QK677jpefPHFOallzr4w5e73AfeZ2YeATwF3AIeAVe4+YGbXAf9kZq+t+gSAu98P3A/Q29t7TrvkJ1s32qMXudCdbs97oTQ2Np54/Pjjj/O9732PH//4xzQ0NPDmN795xnPh0+n0icfxeJyJiYk5qaWWPfoDwMqK5z3hsNk8ANwK4O45dx8IHz8J7AUuO7dST089ehGpp+bmZkZGRmYcNzQ0RHt7Ow0NDTz//PM88cQTC1pbLXv0W4F1ZraWIOBvAz5UOYGZrXP3qc8h7wZ2h8O7gWPuXjKzi4F1wL65Kv7UGoLf2qMXkXro7Ozkhhtu4MorrySbzbJ06dIT4zZs2MBXvvIVLr/8cl7zmtdw/fXXL2htZwx6dy+a2Z3AQ0Ac2OTuO8zsXmCbu28G7jSztwMF4DhB2wbgTcC9ZlYAysBH3P3YfLyRqaBXzotIvXzjG9+YcXg6nebBBx+ccdxUH76rq4vt27efGP7xj398zuqqqUfv7luALVXD7ql4/LFZ5vsu8N3zKbBWU60bV9KLiJwiMt+MnTrpRj16EZFTRSboddaNiMjMIhP06tGLiMwsQkFvmKlHLyJSLTJBD0GfXj16EZFTRSroY2Z4HS6zIyJyrpcpBvjCF77A+Pj4HFd0UuSCXnv0IlIPiznoI3VzcDOddSMi9VF5meIbb7yRJUuW8O1vf5tcLsd73vMePvOZzzA2Nsb73/9++vr6KJVK/NEf/RGHDx/m4MGDvOUtb6Grq4vHHntszmuLXNAr50WEB++CV56d29e86Cq46bOzjq68TPHDDz/Md77zHX7605/i7tx888388Ic/pL+/n+XLl/Nv//ZvQHANnNbWVj7/+c/z2GOP0dXVNbc1hyLXutFZNyJSbw8//DAPP/ww1157La973et4/vnn2b17N1dddRWPPPIIn/zkJ/n3f/93WltbF6SeSO3Rq0cvIsBp97wXgrtz99138zu/8zvTxj311FNs2bKFT33qU7ztbW/jnnvumeEV5lak9ujVoxeReqm8TPE73/lONm3axOjoKAAHDhzgyJEjHDx4kIaGBm6//XY+8YlP8NRTT02bdz5Eao/eUI9eROqj8jLFN910Ex/60Id4wxveAEBTUxN/93d/x549e/jEJz5BLBYjmUzy5S9/GYCNGzeyYcMGli9fPi8HY22x9bR7e3t927Zt5zTvNfc+zC1XL+czt1w5x1WJyGL33HPPcfnll9e7jAUx03s1syfdvXem6SPVulGPXkRkuogFvXr0IiLVIhX0oD16kQvZYmtFz4dzeY+RCvqYAbrWjcgFKZPJMDAwEOmwd3cGBgbIZDJnNV+kzrqJmVEu17sKEamHnp4e+vr66O/vr3cp8yqTydDT03NW89QU9Ga2AfgLgpuD/6W7f7Zq/EeAjwIlYBTY6O47w3F3Ax8Ox/2+uz90VhWeBfXoRS5cyWSStWvX1ruMRemMrRsziwP3ATcBVwAfNLMrqib7hrtf5e7XAJ8DPh/OewVwG/BaYAPwpfD15oXprBsRkWlq6dGvB/a4+z53zwMPALdUTuDuwxVPGznZKL8FeMDdc+7+C2BP+Hrzwgxdj15EpEotrZsVwP6K533Ar1RPZGYfBf4ASAFvrZj3iap5V8ww70ZgI8CqVatqqXtGwUXNznl2EZFImrOzbtz9Pne/BPgk8KmznPd+d+91997u7u5zrkHXuhERma6WoD8ArKx43hMOm80DwK3nOO950TdjRUSmqyXotwLrzGytmaUIDq5urpzAzNZVPH03sDt8vBm4zczSZrYWWAf89PzLnllw4xElvYhIpTP26N29aGZ3Ag8RnF65yd13mNm9wDZ33wzcaWZvBwrAceCOcN4dZvZtYCdQBD7q7qV5ei/q0YuIzKCm8+jdfQuwpWrYPRWPP3aaef8U+NNzLfBsGOrRi4hUi9glELRHLyJSLVJBr7NuRESmi1TQ66wbEZHpIhX0OutGRGS6SAV9zEwXQBARqRKxoFePXkSkWqSCXlevFBGZLmJBrx69iEi1SAW9zqMXEZkuYkGvHr2ISLVIBX3Qo1fQi4hUilbQgw7GiohUiVTQx4J7CYqISIVoBX1MPXoRkWrRCnr16EVEpolU0IN69CIi1SIV9LrWjYjIdBELen0zVkSkWqSCXufRi4hMV1PQm9kGM9tlZnvM7K4Zxv+Bme00s5+b2ffNbHXFuJKZPRP+bJ7L4qvFDMrl+VyCiMirzxlvDm5mceA+4EagD9hqZpvdfWfFZE8Dve4+bma/C3wO+EA4bsLdr5njumerVT16EZEqtezRrwf2uPs+d88DDwC3VE7g7o+5+3j49AmgZ27LrI169CIi09US9CuA/RXP+8Jhs/kw8GDF84yZbTOzJ8zs1plmMLON4TTb+vv7ayhpZoZ69CIi1c7YujkbZnY70Av8esXg1e5+wMwuBh41s2fdfW/lfO5+P3A/QG9v7zkndSyGLlMsIlKllj36A8DKiuc94bBTmNnbgT8Ebnb33NRwdz8Q/t4HPA5cex71npbOuhERma6WoN8KrDOztWaWAm4DTjl7xsyuBb5KEPJHKoa3m1k6fNwF3ABUHsSdU7rxiIjIdGds3bh70czuBB4C4sAmd99hZvcC29x9M/DnQBPw92YG8LK73wxcDnzVzMoEG5XPVp2tM6eCyxQr6UVEKtXUo3f3LcCWqmH3VDx++yzz/Qi46nwKPBsxXaVYRGSaSH0zVlevFBGZLlJBb2b6ZqyISJWIBb2+MCUiUi1SQa8evYjIdBELevXoRUSqRSrogy9M1bsKEZHFJWJBrx69iEi1SAV9cPXKelchIrK4RCzo1aMXEakWwaCvdxUiIotLpIIedK0bEZFqkQr6mOlEehGRahELeu3Ri4hUi1TQm6EevYhIlUgFvc66ERGZLlJBb2Zq0YuIVIlU0Mf0zVgRkWkiFfTq0YuITFdT0JvZBjPbZWZ7zOyuGcb/gZntNLOfm9n3zWx1xbg7zGx3+HPHXBZfLbg5uJJeRKTSGYPezOLAfcBNwBXAB83siqrJngZ63f2Xge8Anwvn7QA+DfwKsB74tJm1z13502rVHr2ISJVa9ujXA3vcfZ+754EHgFsqJ3D3x9x9PHz6BNATPn4n8Ii7H3P348AjwIa5KX26mJ2oZ74WISLyqlNL0K8A9lc87wuHzebDwINnM6+ZbTSzbWa2rb+/v4aSZmYESa+9ehGRk+b0YKyZ3Q70An9+NvO5+/3u3uvuvd3d3ee8fO3Ri4hMV0vQHwBWVjzvCYedwszeDvwhcLO7585m3rkSi2mPXkSkWi1BvxVYZ2ZrzSwF3AZsrpzAzK4FvkoQ8kcqRj0EvMPM2sODsO8Ih80LC/fo9e1YEZGTEmeawN2LZnYnQUDHgU3uvsPM7gW2uftmglZNE/D3FqTty+5+s7sfM7M/IdhYANzr7sfm5Z1wskevnBcROemMQQ/g7luALVXD7ql4/PbTzLsJ2HSuBZ6NEz16XQhBROSESH0zNmbq0YuIVItU0KtHLyIyXcSCPuzRl+tciIjIIhKpoFePXkRkuogFvXr0IiLVIhb0wW/16EVETopU0HNij15BLyIyJVJBP7VHrxa9iMhJEQt69ehFRKpFKuindujVuhEROSlSQR9Tj15EZJpIBb2duB59fesQEVlMIhX0U3v0CnoRkZMiFfS61o2IyHSRCvoTe/R1rkNEZDGJVNBrj15EZLpIBf3JHr2CXkRkSqSC/uQefX3rEBFZTCIV9DrrRkRkupqC3sw2mNkuM9tjZnfNMP5NZvaUmRXN7L1V40pm9kz4s3muCp/JVNAXy7rziIjIlDPeHNzM4sB9wI1AH7DVzDa7+86KyV4Gfhv4+AwvMeHu18xBrac3OcRVT93D9bF1FEs3zPviREReLWrZo18P7HH3fe6eBx4AbqmcwN1fdPefA/XblS6XWLHvW/ySvaw9ehGRCrUE/Qpgf8XzvnBYrTJmts3MnjCzW2eawMw2htNs6+/vP4uXrpDIAJAlT6GkJr2IyJSFOBi72t17gQ8BXzCzS6oncPf73b3X3Xu7u7vPbSlh0GcsT1FBLyJyQi1BfwBYWfG8JxxWE3c/EP7eBzwOXHsW9dUuFqMcT5MmT0GtGxGRE2oJ+q3AOjNba2Yp4DagprNnzKzdzNLh4y7gBmDn6ec6d+VEhgzaoxcRqXTGoHf3InAn8BDwHPBtd99hZvea2c0AZvZ6M+sD3gd81cx2hLNfDmwzs58BjwGfrTpbZ055IkuWPMWS9uhFRKac8fRKAHffAmypGnZPxeOtBC2d6vl+BFx1njXWzBMZMpYnr6AXETkhUt+MJZFV60ZEpErEgj7s0etgrIjICdEK+mSWjOk8ehGRStELeh2MFRE5RaSC3qaCXtcpFhE5IVpBn2ogo0sgiIicIlJBH0tmyZpaNyIilSIV9JbMhHv0CnoRkSnRCvqp1o169CIiJ0Qq6ElkSVmRUrFQ70pERBaNaAV9MrhUMYXJ+tYhIrKIRCvoE9ngd2mivnWIiCwi0Qr6ZBD0VsjVuRARkcUjmkFfVOtGRGRKJIM+XhqvcyEiIotHtII+1QRAvDhW50JERBaPiAa9DsaKiEyJVtCng6BPaY9eROSEaAV9qhGApHr0IiIn1BT0ZrbBzHaZ2R4zu2uG8W8ys6fMrGhm760ad4eZ7Q5/7pirwmekoBcRmeaMQW9mceA+4CbgCuCDZnZF1WQvA78NfKNq3g7g08CvAOuBT5tZ+/mXPYuwR5/SF6ZERE6oZY9+PbDH3fe5ex54ALilcgJ3f9Hdfw5UXzbyncAj7n7M3Y8DjwAb5qDumcWT5EmSKmuPXkRkSi1BvwLYX/G8LxxWi5rmNbONZrbNzLb19/fX+NIzy8UaSJe1Ry8iMmVRHIx19/vdvdfde7u7u8/rtXKxLGnt0YuInFBL0B8AVlY87wmH1eJ85j0nuVgDGe3Ri4icUEvQbwXWmdlaM0sBtwGba3z9h4B3mFl7eBD2HeGweZOPZ8m4gl5EZMoZg97di8CdBAH9HPBtd99hZvea2c0AZvZ6M+sD3gd81cx2hPMeA/6EYGOxFbg3HDZv8rEGBb2ISIVELRO5+xZgS9WweyoebyVoy8w07yZg03nUeFYK8QYa/JWFWpyIyKK3KA7GzqVCooEsukyxiMiUyAV9Md5Ag1o3IiInRC7oy+lWmhinVKr+7paIyIUpckHv2TYSVmZ05Hi9SxERWRQiF/SWDS6lMz54ft+wFRGJisgFfbyxE4DJ4aN1rkREZHGIXNAnmzoAyI8O1LkSEZHFIXJBn2rpAqCgoBcRASIY9JnmIOhLYzoYKyICEQz6xtagR+/j83qlBRGRV43IBX1zYyOjnoGJwXqXIiKyKEQu6DPJGEM0EZ9U60ZEBCIY9GbGiDWRymuPXkQEIhj0AEOxdjJ5nXUjIgIRDfrhRCfNBQW9iAhENOgn0l20lI5BWRc2ExGJZNCXGrpJUIIJnWIpIhLJoKf5IgB8RHeaEhGpKejNbIOZ7TKzPWZ21wzj02b2rXD8T8xsTTh8jZlNmNkz4c9X5rb8mSValwEwfuzgQixORGRRO+M9Y80sDtwH3Aj0AVvNbLO776yY7MPAcXe/1MxuA/4M+EA4bq+7XzPHdZ9Wtj0I+tGjfTQu5IJFRBahWvbo1wN73H2fu+eBB4Bbqqa5Bfjr8PF3gLeZmc1dmWenqSu4T/nksQP1KkFEZNGoJehXAPsrnveFw2acxt2LwBDQGY5ba2ZPm9kPzOzXzrPemnS2tzPgzfjg/jNPLCIScWds3ZynQ8Aqdx8ws+uAfzKz17r7cOVEZrYR2AiwatWq815od3Oal7ybzuGXz/u1RERe7WrZoz8ArKx43hMOm3EaM0sArcCAu+fcfQDA3Z8E9gKXVS/A3e9391537+3u7j77d1GlNZvkcGwp6dG+834tEZFXu1qCfiuwzszWmlkKuA3YXDXNZuCO8PF7gUfd3c2sOzyYi5ldDKwD9s1N6bMzM3JNPbTlD+lLUyJywTtj68bdi2Z2J/AQEAc2ufsOM7sX2Obum4GvA39rZnuAYwQbA4A3AfeaWQEoAx9x9wX5FlOicw3J0SLFoYMk2nsWYpEiIotSTT16d98CbKkadk/F40ngfTPM913gu+dZ4zlpWX4pvAQHf/EcqxT0InIBi+Y3Y4Gey64D4Miep+pciYhIfUU26FetvoRBmikc/Fm9SxERqavIBr3FYrySvZTWoV24e73LERGpm8gGPQBLr+Ti8ks88+KRelciIlI3kQ761de9g4wVeOIHD9a7FBGRuol00Gcvewsl4sT3fZ+hiUK9yxERqYtIBz3pZiaWrecGfsY/PqVvyYrIhSnaQQ80vfadvDb2Ev/6/56mXNZBWRG58EQ+6LnkbQCsGXqCh3bojlMicuGJftBfdBXesoL3ZZ/kv/7bcxwdzdW7IhGRBRX9oDfDfvkDrC89TWzsFXr/6/f48uN7612ViMiCiX7QA1x7O2bG5nUPsrIjy/96dDd7+0frXZWIyIK4MIK+8xL49U/Svm8zf3P9IXLFMjd+/gf85289w7/+/CAlHaQVkQizxXZ5gN7eXt+2bdvcv3CpAF9/BxzewUDv7/NHfdezZW/Qr7+oJcN1a9q58fKl3Hpt9V0SRUQWPzN70t17Zxx3wQQ9wNgAbL4Tdm2BRJbypTeyM3MtXx+4kodfKjOWL3H9xR18cP0qljRnABjPF/m1dd2kEhfGhx8ReXVS0Fc7vBN+8mXY9zgMBveVLa/o5bHMjfz3vl/iuaHkKZN3NaUplstcubyVS5c00ZpNcumSJtoakjSkgkv6N6TiXL6sZX7rFhGZhYJ+Nu7wyrOw+yHY/g9wZCduMSY7LmciexHD7a9lJLWUfz9QYjLVyQ+GL2LPsSJj+dKML7e2q5Hu5jRD4wUGxvIsbUkzlivS097AW35pCS2ZBKWyUyiVuXplG8fHC2STcZa3ZUjEYixpTgMwPFmgJZOkUC6TTsQXZl2IyKuagr4W7nDoGdj1IBx4EoYOQP9z06dLNuINHUxkL2IiexG5WANj2WW8NGIcHDMO5bPQtARv6GLvSJxl2RLbjibZebR4xhK6mtJkkjH6jk+QiBnFsnPtqjaSsRiFcplsMs5YvoS782vruljT2Ug6GWfrL44xMlngqp422huSNKYTNKUTjOdLHBvLcXF3Ex2NKUplJ18sc+mSJjLJOC8PjDMwluPKFa0k47W3pvpHcnQ0pojH7GzWsIjMIwX9ucqNwvhRGD8GI4fg8A6YHIKxfjj+EowdgcnhYJoz8HiKcrIRTzZAfoxJMtDQST7dxjDNFONpDo3BWClBd3srY+UERUux62iBSRIUGpZyvJAgm4wzkivxxECWssfIk6SYaiabSnJwtATUFr5dTekTXx67YlkLI7kCx8cKtDUkyRXLXLm8hcPDOdobkxwezvGrl3QCsPXF4+x6ZZh1S5r5rV9dzeB4gUNDE7jDFctbWNqc4fDIJLlCmWQiRiYRo6MxRUdjikQsxrMHhljWmmF5W5bnXxlmaUuGmBnpRIylLRlGcwUe39XP8rYsl3Q3sbqzgVyxzIHjE4zni+w/Ps4tV69geLLAzkPDXLmilUwizr6jozSmElzUGhxbKZWdXLFMczpBrIYN0tT/g9FckaZ0AjOjVPYTG7MDgxO0ZBI0Z5KUy37Kaw5PFiiXnbaGVE3rvhbujtm5bUhPN+/UZUBOt06KpTKJs9jwy+Jw3kFvZhuAvyC4Ofhfuvtnq8angb8BrgMGgA+4+4vhuLuBDwMl4Pfd/aHTLWtRBX2tijkojEN+HMYHgg3A2FGYGIR0E4wegdww5MeCn0QGipPBtOMDMHEcCpPBsGIOihNQPvMngGr5eCPlZCOGY8VJitlOLJEmbylGk92Uk1k8lmQoB4M56GhtIjVxlGePOt6ygpaGDGOFMiVL8PKIEU9n2Zdr49KGUZ55Jc+gN7GsvYnVS1r52St5dh4PNjSxVJahPBiOY9S6sTlXUxlVdkglYqQTMUYmg/WVTsSIx4x8sUyx7GSTcdLJGDEzYkb424jHjEKpTDoZBNqR4RxmMFkos7w1gwOvDE/S3ZRmcKJAvlgGoDEVZ7JYZlVHA8vbMozmSjzbN4iZ8euXddPekOKpl48zni/S3ZymOZ3k+HietoYkY7kSo7kiy9syrGjLsuvwKBP5IivbGxgYyzM8UaDszkShxPGxAktb07gHG62YGYm4kYgZ2VSchlSCtZ2NZFNxfri7n6t72pgslBgYy/Oz/YOs6mhgslgiGYuxvC3Lmq4G+kdyPPXyIMVSmZUdDeSLZQrh40KpzMvHxskVygyM5XnjpV0sbUnTnEnywuERhicKtGSTlMrOsbE8qUSMq3vaGJwo0JxJMJEv8dLAGMtas1zc3cjR0RyJWIxMMkYsZkyG7c58qczqzkbyxTIxg52HhimUHHenfyRHQyrBay5qZm//KOlEnKOjOS7uaiQeM5oyCV4aGGdFW5arV7ax/cAQu4+M0N6QwsxY0ZZlx8EhxvMlVnc2kEnGKZWcvsFxetoaSCaMF14Zpb0xyZLmDDGDiUKJdCJOcyZBSzbJ8ESBvuMTwMm/rXQi+BtKxWOkkzGOj+U5PJxjZUeWi1oyJOIxBkZzDE0ErdjVnY2MF0pM5IsUSk5Pe5ahiQJHR3LEYkZjKkG+VGZtVyMHB4NldTWlScSNjsYUv3pJ1zn9vzivoDezOPACcCPQB2wFPujuOyum+T3gl939I2Z2G/Aed/+AmV0BfBNYDywHvgdc5u4zN7l5lQb9fCgVoZQ7uREZPgilfDguH7SWvBw8nhwCHEb7g42ExSCeCjYwXgo2QKOHoTARTF8qQLkQvHa2PdgITQ6dV7luMXCnHE/h6TbMi2AxPJakZAkKsSyFeIZsJktufITReCvNzc3kS1COp5nwFLmSUyrkWNGaomgpxsZGGSqlyZRGSWYaGbIWUrEyg2M5xuKtLOtoYfTILyiUjdXpEYYbVnN4Isa4NZCMx2j2EYqFHFYuMBpvYzzWTMESFEhRII7H04wXoav4CjQvY2Jyko5EnuHxSUrJRjrSJQ6MJ+iITRJvbCebjFMa7Wc8vYQDwzmGJ4skYsYl3Y0UiyV2HhpmYnycdV0ZLkqMsjvXyiu5DB2JSY56K12NCVoTeV4eMfYOOZd3Z2hOOoeOj9CYzdDckKG1PEJTokRHbIxnjxRIt3TTmElCucTxeCfpwhDtk/s5UGpnx1CS8XyJ9R2TPDuYINHYQVvpGNd359lul5GNl2jwcQaPH2PraBfdDcaVXTFi8SRHckliiSTd5X72DuSJZ5tY3pIhZs7x8QIvD4xRLDsDhRQ96Qk6mrMUSRAvjHFZeQ8T8WYGjh1jrGk1pfwkfSxhRVcbkyMDvDyYZ3U2x9FSIyPlFC3lEbriIxyzdnKJZsbGx2lhjBYbpzVVZjSzjGQyQ3djjCOTcfoO93NVV4yfjnSyIlvgrWNbSCSS/GXurfx201Zacgf4l3wvo9bE+tYh2koDPGuXsXs4yZKmJMtbk5QGD9JQGibmRX7RfgN7hgwv5ri+K8fA8AgThTJdDJJLtLCztJLJ3CTN5REwI93cxZLEGAeLrTQWj3Oo1Ey+WKanfIA+72ZpYpRrGo/z45ElTJTjTJCmnRFylmbU04CRJNjxKBAnTYEEJcYtS4wyjT5OgQR5kpQIjsElKNLGGD09q/inO994Tv8Hzzfo3wD8sbu/M3x+N4C7/7eKaR4Kp/mxmSWAV4Bu4K7KaSunm215Cvo6cA9C30vhxqMA+dFgwzB8ABo6g8f5seCTRrkIuZFg41AYD54XJiJrpUEAAAhwSURBVMDiwYZmcghiieB1S4Vg41IYD38mIJEOPsWUisHyipPBcC9BPA2xWDBfPBXUEU8H8xYng2WYBcMBko3BMpuXBRtDqv6eLRbUMrWRlFcdx7Dqf9ezYbHg72z2CTjl7yaeCv5eUs2QHwk+gYc7VW5xbPb9VDyWwBMNxPLDeCwJXsa8hFscWlfAyGGsdPJ6W24xiKdxLxMr5Zhceh2Z33303N7maYI+UcP8K4D9Fc/7gF+ZbRp3L5rZENAZDn+iat5p30gys43ARoBVq1bVUJLMKTNIZk4d1tAR/O5+zcLXU4vCZLCBSWaD57F4uGHJBy0zL0PT0mCjAcGGJTdycsNTykMx/N18UdBqS2Yg1RS+/njwHzw3ApnWYDwEG73xgWCjNNWiOtEPN0ikoFyCluUw1Bcc38m2B8d1YglINQafoIq5IFDiyaD2UjH4lJVpC543dJ5s75XDkBruCzZoHRfD4EvBMaRSHrIdwae/yaFgvnRLcCJBsiH4iaeCDXYsETz3UvC+ABq7g+fFfMV7mXo/fvL9F3PBuEQGll0drI+GDjj+YrDxHdofvE6m7WRNk4PBzkEyCy0rYOSV4HhWIhO8ZmN3UNvRF4L3bPHg/TZ0BHUO7Qd37PLfgGN74dgvoLErqKVUCF6n8+LgPb/8RLDep14n2wZNS4I6X/pR8N4T6WC5eLDz0LEW+ncFdcVTwdsu5oN12dAVLL95WfjJuAytPdjYEWhbDa0r4fD24N8vN3ri38smh7D8KDR0YcWgLUMii5Xywb9Z87Lg7y38+7NSsDxzh5YVZFINc/wfJSxhXl71LLn7/cD9EOzR17kceTWo3jBBGERpaF46fVy2PfiZTeclp1/eKeMvralEWpbXNt256Fp3+vGr3zB/y65c/lzsCNRSa8fa049vXzP7uEvfNvu4NefWJgHgNRvOfd4FVsuh9QPAyornPeGwGacJWzetBAdla5lXRETmUS1BvxVYZ2ZrzSwF3AZsrppmM3BH+Pi9wKMeNP83A7eZWdrM1gLrgJ/OTekiIlKLM7Zuwp77ncBDBKdXbnL3HWZ2L7DN3TcDXwf+1sz2AMcINgaE030b2AkUgY+e7owbERGZe/rClIhIBJzurBt9/U1EJOIU9CIiEaegFxGJOAW9iEjELbqDsWbWD7x0Hi/RBZz5cpILT3WdHdV1dhZrXbB4a4taXavdvXumEYsu6M+XmW2b7chzPamus6O6zs5irQsWb20XUl1q3YiIRJyCXkQk4qIY9PfXu4BZqK6zo7rOzmKtCxZvbRdMXZHr0YuIyKmiuEcvIiIVFPQiIhEXmaA3sw1mtsvM9pjZXXWu5UUze9bMnjGzbeGwDjN7xMx2h79PcxeMOa1lk5kdMbPtFcNmrMUCXwzX4c/N7HULXNcfm9mBcL09Y2bvqhh3d1jXLjN75zzWtdLMHjOznWa2w8w+Fg6v6zo7TV11XWdmljGzn5rZz8K6PhMOX2tmPwmX/63wEueElyz/Vjj8J2a2ZoHr+isz+0XF+romHL5gf/vh8uJm9rSZ/Wv4fH7Xl7u/6n8ILp+8F7gYSAE/A66oYz0vAl1Vwz4H3BU+vgv4swWq5U3A64DtZ6oFeBfwIMFN1a4HfrLAdf0x8PEZpr0i/DdNA2vDf+v4PNW1DHhd+LgZeCFcfl3X2Wnqqus6C993U/g4CfwkXA/fBm4Lh38F+N3w8e8BXwkf3wZ8a57W12x1/RXw3hmmX7C//XB5fwB8A/jX8Pm8rq+o7NGvB/a4+z53zwMPALfUuaZqtwB/HT7+a+DWhViou/+Q4B4BtdRyC/A3HngCaDOzZQtY12xuAR5w95y7/wLYQ/BvPh91HXL3p8LHI8BzBPc5rus6O01ds1mQdRa+7/BO7STDHwfeCnwnHF69vqbW43eAt5mduOnuQtQ1mwX72zezHuDdwF+Gz415Xl9RCfqZbmB+uv8E882Bh83sSQtufA6w1N0PhY9fAWa4semCma2WxbAe7ww/Om+qaG/Vpa7wY/K1BHuDi2adVdUFdV5nYRviGeAI8AjBp4dBdy/OsOwTdYXjh4DOhajL3afW15+G6+t/mlm6uq4Zap5rXwD+CxDe9Z1O5nl9RSXoF5s3uvvrgJuAj5rZmypHevA5bFGc17qYagG+DFwCXAMcAv5HvQoxsybgu8B/cvfhynH1XGcz1FX3debuJXe/huCe0OuBX1roGmZSXZeZXQncTVDf64EO4JMLWZOZ/QfgiLs/uZDLjUrQL6qbkLv7gfD3EeAfCf74D099FAx/H6lXfaeppa7r0d0Ph/85y8DXONlqWNC6zCxJEKb/x93/IRxc93U2U12LZZ2FtQwCjwFvIGh9TN2qtHLZJ+oKx7cCAwtU14awBebungP+Nwu/vm4AbjazFwlazG8F/oJ5Xl9RCfpabmC+IMys0cyapx4D7wC2c+oN1O8A/rke9YVmq2Uz8FvhGQjXA0MV7Yp5V9UTfQ/Bepuqa0FuMh/2P78OPOfun68YVdd1Nltd9V5nZtZtZm3h4yxwI8Hxg8eA94aTVa+vqfX4XuDR8BPSQtT1fMXG2gj64JXra97/Hd39bnfvcfc1BDn1qLv/R+Z7fc3lkeR6/hAcNX+BoD/4h3Ws42KCsx1+BuyYqoWgr/Z9YDfwPaBjger5JsFH+gJB7+/Ds9VCcMbBfeE6fBboXeC6/jZc7s/DP/BlFdP/YVjXLuCmeazrjQRtmZ8Dz4Q/76r3OjtNXXVdZ8AvA0+Hy98O3FPx/+CnBAeB/x5Ih8Mz4fM94fiLF7iuR8P1tR34O06embNgf/sVNb6Zk2fdzOv60iUQREQiLiqtGxERmYWCXkQk4hT0IiIRp6AXEYk4Bb2ISMQp6EVEIk5BLyIScf8fKCE4RjisdIUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGRbfhpiYOlq",
        "outputId": "5887f5af-0688-4321-f120-4388b085f01a"
      },
      "source": [
        "# baseline in performance with logistic regression model\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn import svm\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit model on training set\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# make prediction on test set\r\n",
        "yhat = model.predict(X_test)\r\n",
        "# calculate accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7662337662337663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VXN5bZYWXv4b",
        "outputId": "5189d5b5-9f98-4799-ab88-e1e002fe66c8"
      },
      "source": [
        "# train autoencoder for classification with with compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "# encoder level 1\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 2\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 3\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 4\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# bottleneck\r\n",
        "n_bottleneck = round(float(n_inputs) / 2.0)\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder, level 1\r\n",
        "d = Dense(n_inputs)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# decoder level 2\r\n",
        "d = Dense(n_inputs*2)(d)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder_compress.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "44/44 - 3s - loss: 0.5142 - val_loss: 0.1479\n",
            "Epoch 2/200\n",
            "44/44 - 0s - loss: 0.1938 - val_loss: 0.1127\n",
            "Epoch 3/200\n",
            "44/44 - 0s - loss: 0.0817 - val_loss: 0.0805\n",
            "Epoch 4/200\n",
            "44/44 - 0s - loss: 0.0490 - val_loss: 0.0590\n",
            "Epoch 5/200\n",
            "44/44 - 0s - loss: 0.0374 - val_loss: 0.0480\n",
            "Epoch 6/200\n",
            "44/44 - 0s - loss: 0.0312 - val_loss: 0.0430\n",
            "Epoch 7/200\n",
            "44/44 - 0s - loss: 0.0294 - val_loss: 0.0383\n",
            "Epoch 8/200\n",
            "44/44 - 0s - loss: 0.0274 - val_loss: 0.0347\n",
            "Epoch 9/200\n",
            "44/44 - 0s - loss: 0.0269 - val_loss: 0.0322\n",
            "Epoch 10/200\n",
            "44/44 - 0s - loss: 0.0249 - val_loss: 0.0291\n",
            "Epoch 11/200\n",
            "44/44 - 0s - loss: 0.0246 - val_loss: 0.0266\n",
            "Epoch 12/200\n",
            "44/44 - 0s - loss: 0.0233 - val_loss: 0.0254\n",
            "Epoch 13/200\n",
            "44/44 - 0s - loss: 0.0227 - val_loss: 0.0245\n",
            "Epoch 14/200\n",
            "44/44 - 0s - loss: 0.0219 - val_loss: 0.0239\n",
            "Epoch 15/200\n",
            "44/44 - 0s - loss: 0.0214 - val_loss: 0.0234\n",
            "Epoch 16/200\n",
            "44/44 - 0s - loss: 0.0210 - val_loss: 0.0222\n",
            "Epoch 17/200\n",
            "44/44 - 0s - loss: 0.0205 - val_loss: 0.0220\n",
            "Epoch 18/200\n",
            "44/44 - 0s - loss: 0.0201 - val_loss: 0.0211\n",
            "Epoch 19/200\n",
            "44/44 - 0s - loss: 0.0200 - val_loss: 0.0203\n",
            "Epoch 20/200\n",
            "44/44 - 0s - loss: 0.0195 - val_loss: 0.0198\n",
            "Epoch 21/200\n",
            "44/44 - 0s - loss: 0.0194 - val_loss: 0.0195\n",
            "Epoch 22/200\n",
            "44/44 - 0s - loss: 0.0192 - val_loss: 0.0200\n",
            "Epoch 23/200\n",
            "44/44 - 0s - loss: 0.0192 - val_loss: 0.0196\n",
            "Epoch 24/200\n",
            "44/44 - 0s - loss: 0.0186 - val_loss: 0.0192\n",
            "Epoch 25/200\n",
            "44/44 - 0s - loss: 0.0188 - val_loss: 0.0195\n",
            "Epoch 26/200\n",
            "44/44 - 0s - loss: 0.0181 - val_loss: 0.0183\n",
            "Epoch 27/200\n",
            "44/44 - 0s - loss: 0.0188 - val_loss: 0.0178\n",
            "Epoch 28/200\n",
            "44/44 - 0s - loss: 0.0185 - val_loss: 0.0181\n",
            "Epoch 29/200\n",
            "44/44 - 0s - loss: 0.0183 - val_loss: 0.0180\n",
            "Epoch 30/200\n",
            "44/44 - 0s - loss: 0.0179 - val_loss: 0.0176\n",
            "Epoch 31/200\n",
            "44/44 - 0s - loss: 0.0178 - val_loss: 0.0174\n",
            "Epoch 32/200\n",
            "44/44 - 0s - loss: 0.0171 - val_loss: 0.0172\n",
            "Epoch 33/200\n",
            "44/44 - 0s - loss: 0.0177 - val_loss: 0.0172\n",
            "Epoch 34/200\n",
            "44/44 - 0s - loss: 0.0169 - val_loss: 0.0166\n",
            "Epoch 35/200\n",
            "44/44 - 0s - loss: 0.0166 - val_loss: 0.0167\n",
            "Epoch 36/200\n",
            "44/44 - 0s - loss: 0.0171 - val_loss: 0.0159\n",
            "Epoch 37/200\n",
            "44/44 - 0s - loss: 0.0164 - val_loss: 0.0168\n",
            "Epoch 38/200\n",
            "44/44 - 0s - loss: 0.0166 - val_loss: 0.0160\n",
            "Epoch 39/200\n",
            "44/44 - 0s - loss: 0.0165 - val_loss: 0.0153\n",
            "Epoch 40/200\n",
            "44/44 - 0s - loss: 0.0163 - val_loss: 0.0150\n",
            "Epoch 41/200\n",
            "44/44 - 0s - loss: 0.0164 - val_loss: 0.0154\n",
            "Epoch 42/200\n",
            "44/44 - 0s - loss: 0.0162 - val_loss: 0.0154\n",
            "Epoch 43/200\n",
            "44/44 - 0s - loss: 0.0158 - val_loss: 0.0148\n",
            "Epoch 44/200\n",
            "44/44 - 0s - loss: 0.0160 - val_loss: 0.0151\n",
            "Epoch 45/200\n",
            "44/44 - 0s - loss: 0.0159 - val_loss: 0.0151\n",
            "Epoch 46/200\n",
            "44/44 - 0s - loss: 0.0161 - val_loss: 0.0164\n",
            "Epoch 47/200\n",
            "44/44 - 0s - loss: 0.0157 - val_loss: 0.0147\n",
            "Epoch 48/200\n",
            "44/44 - 0s - loss: 0.0152 - val_loss: 0.0144\n",
            "Epoch 49/200\n",
            "44/44 - 0s - loss: 0.0154 - val_loss: 0.0145\n",
            "Epoch 50/200\n",
            "44/44 - 0s - loss: 0.0151 - val_loss: 0.0148\n",
            "Epoch 51/200\n",
            "44/44 - 0s - loss: 0.0160 - val_loss: 0.0149\n",
            "Epoch 52/200\n",
            "44/44 - 0s - loss: 0.0151 - val_loss: 0.0143\n",
            "Epoch 53/200\n",
            "44/44 - 0s - loss: 0.0152 - val_loss: 0.0135\n",
            "Epoch 54/200\n",
            "44/44 - 0s - loss: 0.0151 - val_loss: 0.0139\n",
            "Epoch 55/200\n",
            "44/44 - 0s - loss: 0.0153 - val_loss: 0.0136\n",
            "Epoch 56/200\n",
            "44/44 - 0s - loss: 0.0149 - val_loss: 0.0141\n",
            "Epoch 57/200\n",
            "44/44 - 0s - loss: 0.0147 - val_loss: 0.0136\n",
            "Epoch 58/200\n",
            "44/44 - 0s - loss: 0.0141 - val_loss: 0.0133\n",
            "Epoch 59/200\n",
            "44/44 - 0s - loss: 0.0148 - val_loss: 0.0135\n",
            "Epoch 60/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0134\n",
            "Epoch 61/200\n",
            "44/44 - 0s - loss: 0.0144 - val_loss: 0.0138\n",
            "Epoch 62/200\n",
            "44/44 - 0s - loss: 0.0144 - val_loss: 0.0136\n",
            "Epoch 63/200\n",
            "44/44 - 0s - loss: 0.0144 - val_loss: 0.0137\n",
            "Epoch 64/200\n",
            "44/44 - 0s - loss: 0.0143 - val_loss: 0.0135\n",
            "Epoch 65/200\n",
            "44/44 - 0s - loss: 0.0150 - val_loss: 0.0142\n",
            "Epoch 66/200\n",
            "44/44 - 0s - loss: 0.0144 - val_loss: 0.0139\n",
            "Epoch 67/200\n",
            "44/44 - 0s - loss: 0.0145 - val_loss: 0.0132\n",
            "Epoch 68/200\n",
            "44/44 - 0s - loss: 0.0145 - val_loss: 0.0137\n",
            "Epoch 69/200\n",
            "44/44 - 0s - loss: 0.0143 - val_loss: 0.0137\n",
            "Epoch 70/200\n",
            "44/44 - 0s - loss: 0.0142 - val_loss: 0.0135\n",
            "Epoch 71/200\n",
            "44/44 - 0s - loss: 0.0142 - val_loss: 0.0133\n",
            "Epoch 72/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0130\n",
            "Epoch 73/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0133\n",
            "Epoch 74/200\n",
            "44/44 - 0s - loss: 0.0139 - val_loss: 0.0131\n",
            "Epoch 75/200\n",
            "44/44 - 0s - loss: 0.0134 - val_loss: 0.0131\n",
            "Epoch 76/200\n",
            "44/44 - 0s - loss: 0.0136 - val_loss: 0.0129\n",
            "Epoch 77/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0132\n",
            "Epoch 78/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0131\n",
            "Epoch 79/200\n",
            "44/44 - 0s - loss: 0.0134 - val_loss: 0.0136\n",
            "Epoch 80/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0133\n",
            "Epoch 81/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0137\n",
            "Epoch 82/200\n",
            "44/44 - 0s - loss: 0.0137 - val_loss: 0.0130\n",
            "Epoch 83/200\n",
            "44/44 - 0s - loss: 0.0134 - val_loss: 0.0129\n",
            "Epoch 84/200\n",
            "44/44 - 0s - loss: 0.0135 - val_loss: 0.0131\n",
            "Epoch 85/200\n",
            "44/44 - 0s - loss: 0.0140 - val_loss: 0.0132\n",
            "Epoch 86/200\n",
            "44/44 - 0s - loss: 0.0132 - val_loss: 0.0138\n",
            "Epoch 87/200\n",
            "44/44 - 0s - loss: 0.0135 - val_loss: 0.0136\n",
            "Epoch 88/200\n",
            "44/44 - 0s - loss: 0.0138 - val_loss: 0.0131\n",
            "Epoch 89/200\n",
            "44/44 - 0s - loss: 0.0132 - val_loss: 0.0133\n",
            "Epoch 90/200\n",
            "44/44 - 0s - loss: 0.0131 - val_loss: 0.0128\n",
            "Epoch 91/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0124\n",
            "Epoch 92/200\n",
            "44/44 - 0s - loss: 0.0134 - val_loss: 0.0120\n",
            "Epoch 93/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0125\n",
            "Epoch 94/200\n",
            "44/44 - 0s - loss: 0.0127 - val_loss: 0.0121\n",
            "Epoch 95/200\n",
            "44/44 - 0s - loss: 0.0133 - val_loss: 0.0123\n",
            "Epoch 96/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0123\n",
            "Epoch 97/200\n",
            "44/44 - 0s - loss: 0.0131 - val_loss: 0.0128\n",
            "Epoch 98/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0131\n",
            "Epoch 99/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0129\n",
            "Epoch 100/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0122\n",
            "Epoch 101/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0122\n",
            "Epoch 102/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0125\n",
            "Epoch 103/200\n",
            "44/44 - 0s - loss: 0.0131 - val_loss: 0.0123\n",
            "Epoch 104/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0124\n",
            "Epoch 105/200\n",
            "44/44 - 0s - loss: 0.0127 - val_loss: 0.0121\n",
            "Epoch 106/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0121\n",
            "Epoch 107/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0153\n",
            "Epoch 108/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0135\n",
            "Epoch 109/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0119\n",
            "Epoch 110/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0117\n",
            "Epoch 111/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0114\n",
            "Epoch 112/200\n",
            "44/44 - 0s - loss: 0.0130 - val_loss: 0.0118\n",
            "Epoch 113/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0115\n",
            "Epoch 114/200\n",
            "44/44 - 0s - loss: 0.0127 - val_loss: 0.0113\n",
            "Epoch 115/200\n",
            "44/44 - 0s - loss: 0.0127 - val_loss: 0.0118\n",
            "Epoch 116/200\n",
            "44/44 - 0s - loss: 0.0124 - val_loss: 0.0115\n",
            "Epoch 117/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0120\n",
            "Epoch 118/200\n",
            "44/44 - 0s - loss: 0.0121 - val_loss: 0.0122\n",
            "Epoch 119/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0119\n",
            "Epoch 120/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0135\n",
            "Epoch 121/200\n",
            "44/44 - 0s - loss: 0.0119 - val_loss: 0.0117\n",
            "Epoch 122/200\n",
            "44/44 - 0s - loss: 0.0129 - val_loss: 0.0116\n",
            "Epoch 123/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0115\n",
            "Epoch 124/200\n",
            "44/44 - 0s - loss: 0.0121 - val_loss: 0.0114\n",
            "Epoch 125/200\n",
            "44/44 - 0s - loss: 0.0125 - val_loss: 0.0113\n",
            "Epoch 126/200\n",
            "44/44 - 0s - loss: 0.0126 - val_loss: 0.0111\n",
            "Epoch 127/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0114\n",
            "Epoch 128/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0113\n",
            "Epoch 129/200\n",
            "44/44 - 0s - loss: 0.0123 - val_loss: 0.0110\n",
            "Epoch 130/200\n",
            "44/44 - 0s - loss: 0.0121 - val_loss: 0.0108\n",
            "Epoch 131/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0113\n",
            "Epoch 132/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0109\n",
            "Epoch 133/200\n",
            "44/44 - 0s - loss: 0.0119 - val_loss: 0.0110\n",
            "Epoch 134/200\n",
            "44/44 - 0s - loss: 0.0119 - val_loss: 0.0110\n",
            "Epoch 135/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0106\n",
            "Epoch 136/200\n",
            "44/44 - 0s - loss: 0.0120 - val_loss: 0.0107\n",
            "Epoch 137/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0112\n",
            "Epoch 138/200\n",
            "44/44 - 0s - loss: 0.0122 - val_loss: 0.0115\n",
            "Epoch 139/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0110\n",
            "Epoch 140/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0112\n",
            "Epoch 141/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0107\n",
            "Epoch 142/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0106\n",
            "Epoch 143/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0107\n",
            "Epoch 144/200\n",
            "44/44 - 0s - loss: 0.0118 - val_loss: 0.0109\n",
            "Epoch 145/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0108\n",
            "Epoch 146/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0107\n",
            "Epoch 147/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0106\n",
            "Epoch 148/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0108\n",
            "Epoch 149/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0102\n",
            "Epoch 150/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0104\n",
            "Epoch 151/200\n",
            "44/44 - 0s - loss: 0.0120 - val_loss: 0.0104\n",
            "Epoch 152/200\n",
            "44/44 - 0s - loss: 0.0115 - val_loss: 0.0102\n",
            "Epoch 153/200\n",
            "44/44 - 0s - loss: 0.0114 - val_loss: 0.0105\n",
            "Epoch 154/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0101\n",
            "Epoch 155/200\n",
            "44/44 - 0s - loss: 0.0116 - val_loss: 0.0100\n",
            "Epoch 156/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0102\n",
            "Epoch 157/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0109\n",
            "Epoch 158/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0100\n",
            "Epoch 159/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0101\n",
            "Epoch 160/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0101\n",
            "Epoch 161/200\n",
            "44/44 - 0s - loss: 0.0117 - val_loss: 0.0102\n",
            "Epoch 162/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0097\n",
            "Epoch 163/200\n",
            "44/44 - 0s - loss: 0.0113 - val_loss: 0.0103\n",
            "Epoch 164/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0095\n",
            "Epoch 165/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0093\n",
            "Epoch 166/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0094\n",
            "Epoch 167/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0095\n",
            "Epoch 168/200\n",
            "44/44 - 0s - loss: 0.0112 - val_loss: 0.0095\n",
            "Epoch 169/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0095\n",
            "Epoch 170/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0094\n",
            "Epoch 171/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0090\n",
            "Epoch 172/200\n",
            "44/44 - 0s - loss: 0.0104 - val_loss: 0.0090\n",
            "Epoch 173/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0094\n",
            "Epoch 174/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0090\n",
            "Epoch 175/200\n",
            "44/44 - 0s - loss: 0.0111 - val_loss: 0.0090\n",
            "Epoch 176/200\n",
            "44/44 - 0s - loss: 0.0107 - val_loss: 0.0096\n",
            "Epoch 177/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0089\n",
            "Epoch 178/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0089\n",
            "Epoch 179/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0089\n",
            "Epoch 180/200\n",
            "44/44 - 0s - loss: 0.0110 - val_loss: 0.0090\n",
            "Epoch 181/200\n",
            "44/44 - 0s - loss: 0.0105 - val_loss: 0.0094\n",
            "Epoch 182/200\n",
            "44/44 - 0s - loss: 0.0108 - val_loss: 0.0093\n",
            "Epoch 183/200\n",
            "44/44 - 0s - loss: 0.0102 - val_loss: 0.0088\n",
            "Epoch 184/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0090\n",
            "Epoch 185/200\n",
            "44/44 - 0s - loss: 0.0106 - val_loss: 0.0092\n",
            "Epoch 186/200\n",
            "44/44 - 0s - loss: 0.0101 - val_loss: 0.0090\n",
            "Epoch 187/200\n",
            "44/44 - 0s - loss: 0.0104 - val_loss: 0.0091\n",
            "Epoch 188/200\n",
            "44/44 - 0s - loss: 0.0103 - val_loss: 0.0089\n",
            "Epoch 189/200\n",
            "44/44 - 0s - loss: 0.0105 - val_loss: 0.0085\n",
            "Epoch 190/200\n",
            "44/44 - 0s - loss: 0.0099 - val_loss: 0.0086\n",
            "Epoch 191/200\n",
            "44/44 - 0s - loss: 0.0100 - val_loss: 0.0084\n",
            "Epoch 192/200\n",
            "44/44 - 0s - loss: 0.0101 - val_loss: 0.0084\n",
            "Epoch 193/200\n",
            "44/44 - 0s - loss: 0.0100 - val_loss: 0.0084\n",
            "Epoch 194/200\n",
            "44/44 - 0s - loss: 0.0103 - val_loss: 0.0084\n",
            "Epoch 195/200\n",
            "44/44 - 0s - loss: 0.0100 - val_loss: 0.0084\n",
            "Epoch 196/200\n",
            "44/44 - 0s - loss: 0.0103 - val_loss: 0.0084\n",
            "Epoch 197/200\n",
            "44/44 - 0s - loss: 0.0096 - val_loss: 0.0084\n",
            "Epoch 198/200\n",
            "44/44 - 0s - loss: 0.0101 - val_loss: 0.0085\n",
            "Epoch 199/200\n",
            "44/44 - 0s - loss: 0.0102 - val_loss: 0.0085\n",
            "Epoch 200/200\n",
            "44/44 - 0s - loss: 0.0100 - val_loss: 0.0083\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgN5VaavYhVx",
        "outputId": "3b8942a0-bdb9-4664-cc90-3d07cf0ae4a7"
      },
      "source": [
        "# evaluate logistic regression on encoded input\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "# define dataset\r\n",
        "#X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "dataset = loadtxt('/content/drive/MyDrive/Classification/dia.csv', delimiter=',')\r\n",
        "X = dataset[:,0:8]\r\n",
        "y = dataset[:,8]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# load the model from file\r\n",
        "encoder = load_model('encoder.h5')\r\n",
        "# encode the train data\r\n",
        "X_train_encode = encoder.predict(X_train)\r\n",
        "# encode the test data\r\n",
        "X_test_encode = encoder.predict(X_test)\r\n",
        "# define the model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit the model on the training set\r\n",
        "model.fit(X_train_encode, y_train)\r\n",
        "# make predictions on the test set\r\n",
        "yhat = model.predict(X_test_encode)\r\n",
        "# calculate classification accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0.7012987012987013\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}